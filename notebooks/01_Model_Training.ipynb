{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this project is to generate a segmentation mask for neurons that differentiates the soma and neurites from the background.\n",
    "This primarily useful for automation features, such as morphological measurements, cell counting, etc.\n",
    "We'll be doing this using a variant of U-net, which is a common choice for segmentation tasks in cellular biology. With the model defined,\n",
    "we then train it on the synthetically generated data.\n",
    "\n",
    "*Note: You can use the outline of this notebook to navigate the sections.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The following sections are for setting up the environment to load the ata and run the model.\n",
    "\n",
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from random import Random\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import matplotlib. pyplot as plt \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2.functional as FT\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "\n",
    "# Local module for configuration data.\n",
    "import userconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging module with desired format and level\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Create a logger for this notebook\n",
    "logger = logging.getLogger('segmentation-notebook')\n",
    "logger.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture and Implementation\n",
    "\n",
    "The U-net model, in general, works by successively encoding features until the spatial resolution has been downsized by factor of 4 or 8.\n",
    "After that, the model progressively up samples and concatenates features from prior encoding states. It can be thought of as an auto encoder\n",
    "with skip connections. There are several variations, such as the addition of residual blocks or residual connections, but we will be using\n",
    "the foundational version. The nice thing about U-net is that it is fully convolutional, so even if we train with a small spatial resolution,\n",
    "we can use larger spatial resolutions during deployment.\n",
    "\n",
    "Here is a diagram that illustrates what is happening visually.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../docs/unet.png\" />\n",
    "</p>\n",
    "\n",
    "Many variants of U-net will go deeper, and many will also have batch normalization and activation functions along side the convolutions.\n",
    "This diagram is a slight simplification in that it only includes the convolution, interpolation, and concatenation functions of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Used for down sizing the input and encoding new features.\n",
    "    This modules all have a stride of 2, a kernel size of 3, and a padding of 1.\n",
    "    So any input signal that is passed to it will have an output whose spatial\n",
    "    resolution is half of that of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the encoder.\n",
    "\n",
    "        :param in_channels: The number of input channels to the module.\n",
    "        :param out_channels: The number of output channels to return by the module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Runs the forward pass of the module.\n",
    "\n",
    "        :param x: The input signal to the module.\n",
    "\n",
    "        :returns: The down sampled output of the module, pairsed with the original input signal.\n",
    "                  The original input signal may be used later for skip connections to the decoders.\n",
    "        \"\"\"\n",
    "        skip = x\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        return x, skip\n",
    "\n",
    "def test_encoder():\n",
    "    enc = _Encoder(16, 32)\n",
    "    x = torch.randn((4, 16, 128, 128))\n",
    "    y, skip = enc(x)\n",
    "    assert(y.shape[0] == 4)\n",
    "    assert(y.shape[1] == 32)\n",
    "    assert(y.shape[2] == 64)\n",
    "    assert(y.shape[3] == 64)\n",
    "    assert(skip.shape == x.shape)\n",
    "    logger.info('Encoder works.')\n",
    "\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This module is used for up sampling the decoded feature maps by\n",
    "    a factor of two and reconstructing detail. It also handles the\n",
    "    concatenation of previous feature maps (in other words, handles\n",
    "    the skip connections).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the decoder module.\n",
    "\n",
    "        :param in_channels: The number of input channels for this module.\n",
    "                            Note that this should include the number of channels\n",
    "                            of the signal being concatenated.\n",
    "        :param output_channels: The number of output channels to return from this module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor, skip: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Runs the forward pass of this module.\n",
    "\n",
    "        :param x: The input signal to upsample.\n",
    "        :param skip: The skip connection to concatenate to the input signal.\n",
    "\n",
    "        :returns: The output of the module.\n",
    "        \"\"\"\n",
    "        shape = (skip.shape[2], skip.shape[3])\n",
    "        x = F.interpolate(x, size=shape, mode='bilinear')\n",
    "        x = torch.concat((x, skip), dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "def test_decoder():\n",
    "    x = torch.randn((4, 16, 32, 32))\n",
    "    skip = torch.randn((4, 8, 64, 64))\n",
    "    dec = _Decoder(24, 4)\n",
    "    y = dec(x, skip)\n",
    "    assert(y.shape[0] == 4)\n",
    "    assert(y.shape[1] == 4)\n",
    "    assert(y.shape[2] == 64)\n",
    "    assert(y.shape[3] == 64)\n",
    "    logger.info('Decoder works.')\n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for building the architecture of the U-net model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features: int = 1):\n",
    "        \"\"\"\n",
    "        Initializes the network.\n",
    "\n",
    "        :param input_features: The number of channels used by the input image. For microscopy, this is generally single\n",
    "                               channel, but it is sometimes 3 channels for widefield microscopy. For confocal microscopy,\n",
    "                               this may be many channels, where each represents a thin slice of the specimen along the Z axis.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.input_features = input_features\n",
    "            self.enc1 = _Encoder(input_features, 16)\n",
    "            self.enc2 = _Encoder(16, 64)\n",
    "            self.enc3 = _Encoder(64, 128)\n",
    "            self.enc4 = _Encoder(128, 256)\n",
    "            self.dec4 = _Decoder(256 + 128, 128)\n",
    "            self.dec3 = _Decoder(128 + 64, 64)\n",
    "            self.dec2 = _Decoder(64 + 16, 32)\n",
    "            self.dec1 = _Decoder(32 + 1, 32)\n",
    "            self.last = nn.Sequential(\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            )\n",
    "            logger.info(\"U-net initialization done successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing U-net: {e}\")\n",
    "            raise    \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\" \n",
    "        Implementation of the U-net logic, in which, the input passes through every step of the architecture.\n",
    "\n",
    "        :param x: Input image from microscope.\n",
    "\n",
    "        :returns: The segmentation mask of the input image, at the same spatial resolution.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            x, skip0 = self.enc1(x)\n",
    "            x, skip1 = self.enc2(x)\n",
    "            x, skip2 = self.enc3(x)\n",
    "            x, skip3 = self.enc4(x)\n",
    "            x = self.dec4(x, skip3)\n",
    "            x = self.dec3(x, skip2)\n",
    "            x = self.dec2(x, skip1)\n",
    "            x = self.dec1(x, skip0)\n",
    "            x = self.last(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error implementing U-net logic: {e}\")\n",
    "            raise \n",
    "\n",
    "    def smoke_test(self, res: tuple[int, int] = (512, 512), batch_size: int = 4):\n",
    "        \"\"\"\n",
    "        This module is for verifying that the network correctly handles the input tensor.\n",
    "        It primarily catches shape errors and input data type errors.\n",
    "\n",
    "        :param res: The spatial resolution to test with.\n",
    "        :param batch_size: The batch size to test with.\n",
    "        \"\"\"\n",
    "        x = torch.randn((batch_size, self.input_features, res[0], res[1]))\n",
    "        self.eval()\n",
    "        try:\n",
    "            y: Tensor = self.forward(x)\n",
    "            assert(y.shape[0] == batch_size)\n",
    "            assert(y.shape[1] == 3)\n",
    "            assert(y.shape[2] == res[0])\n",
    "            assert(y.shape[3] == res[1])\n",
    "            logger.info('U-net works.')\n",
    "        except Exception as e:\n",
    "            logger.error(f'Failed to run basic network test: {e}')\n",
    "            raise\n",
    "\n",
    "def check_unet():\n",
    "    model = UNet()\n",
    "    model.smoke_test()\n",
    "check_unet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "This section goes into loading the data and performing data augmentations to help model generalization.\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class _Sample:\n",
    "    \"\"\"\n",
    "    This class stores one training sample of the dataset.\n",
    "    It consists of an input image and the target segmentation mask.\n",
    "    The class will either store the path or the tensor. Initially,\n",
    "    the path is only stored. Once the sample is used, the image is\n",
    "    opened and the path is replaced with the image tensor. This is\n",
    "    essentially lazy loading and speeds up training after the first\n",
    "    epoch without having to wait for the whole dataset to be loaded\n",
    "    before starting the training loop.\n",
    "    \"\"\"\n",
    "    image: Path | Tensor\n",
    "    target: Path | Tensor\n",
    "\n",
    "class _IdentityTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a null-object that is used in place of an actual transform in the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, target: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        return x, target\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a dataset for training the segmentation network.\n",
    "    It reads a directory containing input images and segmentation\n",
    "    mask images. The input images should be PNG files with 5 digits\n",
    "    to indicate the index of the image (with zero padding). For\n",
    "    example: \"00000.png\". Each one of this images should have a\n",
    "    corresponding \"{index:05}_mask.png\", which is the target segmentation\n",
    "    mask that the network should learn to reproduce. For example,\n",
    "    the input image \"00123.png\" should have a corresponding \"00123_mask.png\".\n",
    "    The segmentation mask should consist of three channels: red, green and blue.\n",
    "    Red represents the soma, green represents a neurite, and blue represents\n",
    "    the background.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir: Path, cache: bool, transform):\n",
    "        \"\"\"\n",
    "        Initialize the segmentation dataset.\n",
    "\n",
    "        :param img_dir: Image directory containing input images and target segmentation masks.\n",
    "        :param cache: Whether or not to cache the images in memory.\n",
    "        :param transform: An optional data transformation function. Must be callable.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.cache = cache\n",
    "            self.samples: list[_Sample] = []\n",
    "            self.transform = transform if transform is not None else _IdentityTransform()\n",
    "            for entry in img_dir.glob('*'):\n",
    "                if 'mask' in entry.stem:\n",
    "                    continue\n",
    "                index = int(str(entry.stem))\n",
    "                mask_entry = img_dir / f'{index:05}_mask.png'\n",
    "                self.samples.append(_Sample(image=entry, target=mask_entry))\n",
    "\n",
    "            logger.info(f\"Segmentation dataset initialization done successfully (num_samples={len(self.samples)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing segmentation dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Gets an input image and the target segmentation mask.\n",
    "\n",
    "        :index: The index of the sample in the dataset.\n",
    "\n",
    "        :returns: An input image and a target segmentation mask.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sample = self.samples[index]\n",
    "            if self.cache:\n",
    "                if isinstance(sample.image, Path):\n",
    "                    sample.image = read_image(str(sample.image)).float() * (1.0 / 255.0)\n",
    "                if isinstance(sample.target, Path):\n",
    "                    sample.target = read_image(str(sample.target)).float() * (1.0 / 255.0)\n",
    "                image = sample.image\n",
    "                target = sample.target\n",
    "                return self.transform(image, target)\n",
    "            else:\n",
    "                image = read_image(str(sample.image)) * (1.0 / 255.0)\n",
    "                target = read_image(str(sample.target)) * (1.0 / 255.0)\n",
    "                return self.transform(image, target)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting sample: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of input and output pairs.\n",
    "\n",
    "        :returns: The number of input and output pairs.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationTransform:\n",
    "    def __init__(self, seed: int):\n",
    "        self.rng = Random(seed)\n",
    "\n",
    "    def __call__(self, x: Tensor, target: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        if self.rng.randint(0, 1) == 1:\n",
    "            x = FT.horizontal_flip(x)\n",
    "            target = FT.horizontal_flip(target)\n",
    "        if self.rng.randint(0, 1) == 1:\n",
    "            x = FT.vertical_flip(x)\n",
    "            target = FT.vertical_flip(target)\n",
    "        if self.rng.randint(0, 1) == 1:\n",
    "            ks = 3 + self.rng.randint(1, 3) * 2\n",
    "            x = FT.gaussian_blur(x, kernel_size=[ks, ks], sigma=[self.rng.uniform(1.0, 9.0)])\n",
    "        if self.rng.randint(0, 1) == 1:\n",
    "            n = torch.randn_like(x)\n",
    "            x = x + (n - x) * self.rng.uniform(0.01, 0.1)\n",
    "        return x, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Augmentation\n",
    "\n",
    "To make sure the data that is getting passed to the network is not completely messed up,\n",
    "we verify by inspection that it is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_augmentation():\n",
    "    with torch.no_grad():\n",
    "        seed = int(time.time())\n",
    "        data = SegmentationDataset(Path('../data/imagery/val'), cache=False, transform=AugmentationTransform(seed=seed))\n",
    "        index = Random(seed).randint(0, len(data) - 1)\n",
    "        x, target = data[index]\n",
    "    plt.imshow(x[0])\n",
    "    plt.title('Augmentation Test')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "test_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard\n",
    "\n",
    "\n",
    "class TrainingLoop(object):\n",
    "    def __init__(self,\n",
    "                 config: userconfig.UserConfig,\n",
    "                 model: nn.Module,\n",
    "                 optimizer: optim.Optimizer,\n",
    "                 train_loader: DataLoader,\n",
    "                 val_loader: DataLoader,\n",
    "                 device):\n",
    "        \"\"\"\n",
    "        Initialize the model training object.\n",
    "\n",
    "        :model: U-net model.\n",
    "        :criterion: Measures the error between the output and the real value.\n",
    "        :optimizer : Adjusts model weights for loss minimization.\n",
    "        :train_loader: Training batches.\n",
    "        :val_loader: Validation batches.\n",
    "        :device: Where the model and data will be processed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.device = device\n",
    "            self.tb_writer = torch.utils.tensorboard.SummaryWriter(config.tensorboard_dir)\n",
    "            self.best_metric = float('inf')\n",
    "            self.best_epoch = -1\n",
    "            self.user_config = config\n",
    "\n",
    "            logger.info('Training object initialization done successfully')\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f'Error initializing training object: {e}')\n",
    "\n",
    "    def train(self, num_epochs: int = 100):\n",
    "        \"\"\"\n",
    "        Train the U-net model\n",
    "\n",
    "        :param num_epochs: The number of epochs to train the model for.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model.train()\n",
    "\n",
    "            best_metrics: dict[str, float] | None = None\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                running_loss = 0.0\n",
    "                count = 0\n",
    "                loader = tqdm(self.train_loader)\n",
    "                for i, (image, target) in enumerate(loader):\n",
    "                    image: Tensor = image.to(self.device)\n",
    "                    target: Tensor = target.to(self.device)\n",
    "\n",
    "                    predicted = self.model(image)\n",
    "                    loss: Tensor = F.cross_entropy(predicted, target)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    count += 1\n",
    "                    loader.set_description(f'Train Loss: {running_loss / count}')\n",
    "\n",
    "                metrics = self.__validate()\n",
    "\n",
    "                self.__log_to_tensorboard(metrics, epoch)\n",
    "\n",
    "                if running_loss < self.best_metric:\n",
    "                    self.model.cpu()\n",
    "                    self.__export_onnx()\n",
    "                    best_metrics = metrics\n",
    "                    self.model.to(self.device)\n",
    "\n",
    "            with mlflow.start_run() as run:\n",
    "                logger.info(run.info.run_id)\n",
    "                if best_metrics is not None:\n",
    "                    for metric_name, value in best_metrics.items():\n",
    "                        mlflow.log_metric(metric_name, value)\n",
    "                mlflow.register_model(model_uri = f'runs:/{run.info.run_id}/neuron-segmentation',\n",
    "                                      name=self.user_config.model_name)\n",
    "\n",
    "            logger.info('Training done.')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error training model: {e}')\n",
    "\n",
    "    def __log_to_tensorboard(self, metrics: dict[str, float], epoch: int):\n",
    "        \"\"\"\n",
    "        Log images and metrics on tensorboard to monitor model training.\n",
    "\n",
    "        :param epoch: The index of the current epoch to tag each image and scalar with.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Get a batch from the validation set\n",
    "                image, target = next(iter(self.val_loader))\n",
    "                image: Tensor = image.to(self.device)\n",
    "                target: Tensor = target.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                predicted = self.model(image)\n",
    "                grid = make_grid(torch.concat((image.expand(-1, 3, -1, -1), predicted, target), dim=0), nrow=4)\n",
    "                self.tb_writer.add_image(f'Result', grid, epoch)\n",
    "                for metric, value in metrics.items():\n",
    "                    self.tb_writer.add_scalar(metric, value, epoch)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error logging on tensorboard: {e}')\n",
    "\n",
    "    def __export_onnx(self):\n",
    "        path = self.user_config.best_model_path\n",
    "        x = torch.randn(1, 1, 512, 512, requires_grad=True)\n",
    "        torch_out = self.model(x)\n",
    "        torch.onnx.export(self.model,\n",
    "                          (x,),\n",
    "                          path,export_params=True,\n",
    "                          opset_version=11,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'],\n",
    "                          dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})\n",
    "\n",
    "    @staticmethod\n",
    "    def __compute_accuracy(prediction: torch.Tensor, target: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Computes the accuracy of the prediction.\n",
    "\n",
    "        :param pred: Predicted class scores, shape [N, 3, H, W].\n",
    "        :param target: Ground truth, shape [N, 3, H, W].\n",
    "\n",
    "        :returns: Dice score per class.\n",
    "        \"\"\"\n",
    "        correct = (prediction.argmax(dim=1) == target.argmax(dim=1)).sum().item()\n",
    "        total = target.shape[0] * target.shape[2] * target.shape[3]\n",
    "        return correct / total\n",
    "\n",
    "    def __validate(self) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Validates training by calculating some metrics that were defined.\n",
    "\n",
    "        :returns: Metrics used to evaluate training.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                val_loop = tqdm(self.val_loader)\n",
    "                count = 0\n",
    "                accuracy_sum = 0.0\n",
    "                for (image, target) in val_loop:\n",
    "                    image: Tensor = image.to(self.device)\n",
    "                    target: Tensor = target.to(self.device)\n",
    "                    predicted = self.model(image)\n",
    "                    loss: Tensor = F.cross_entropy(predicted, target)\n",
    "                    accuracy = TrainingLoop.__compute_accuracy(predicted, target)\n",
    "                    accuracy_sum += accuracy\n",
    "                    val_loss += loss.item()\n",
    "                    count += 1\n",
    "                    val_loop.set_description(f'Validation: Loss = {val_loss / count}')\n",
    "                val_loop.close()\n",
    "                accuracy_avg = accuracy_sum / count\n",
    "                return {\n",
    "                    'val_loss': val_loss / len(self.val_loader),\n",
    "                    'accuracy': accuracy_avg\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f'Error validating training: {e}')\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics used are:\n",
    "\n",
    "|          Metric | Description\n",
    "|-----------------|------------\n",
    "|      Train Loss | Evaluates the error in the data on which the model is learning. A decreasing loss means that the model is learning and improving\n",
    "| Validation Loss | Evaluates the performance of the model on the validation set. It indicates how well the model can generalize to new data. In this case, it measures the error on the validation data. The lower the validation loss value, the better, meaning that the model is generalizing well.\n",
    "|  Class Accuracy | Evaluates how many pixels were accurately classified. Note that this is slightly biased due to the relatively large number of background pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = userconfig.open_user_config()\n",
    "\n",
    "mlflow.set_tracking_uri(config.mlflow_tracking_uri)\n",
    "mlflow.set_experiment(config.mlflow_experiment_name)\n",
    "\n",
    "device = torch.device(config.device)\n",
    "logger.info(f'Using libtorch device: {device}')\n",
    "\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = SegmentationDataset(Path(config.imagery_dir) / 'train',\n",
    "                                    config.cache_data,\n",
    "                                    AugmentationTransform(config.seed))\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = SegmentationDataset(Path(config.imagery_dir) / 'val',\n",
    "                                  config.cache_data,\n",
    "                                  transform=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "model = UNet(input_features=1)\n",
    "optimizer = optim.AdamW(model.parameters()  )\n",
    "trainer = TrainingLoop(config, model, optimizer, train_loader, val_loader, device)\n",
    "trainer.train(config.num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(input_features=1)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "x_batch, y_batch = next(iter(val_loader))\n",
    "x, y = x_batch[0], y_batch[0]\n",
    "x = x.to(device).unsqueeze(dim=0)\n",
    "y = y.unsqueeze(dim=0)\n",
    "pred = F.softmax(model(x))\n",
    "pred = pred.cpu()\n",
    "\n",
    "# Convert tensors to numpy arrays \n",
    "y = y[0].numpy().transpose(1, 2, 0)\n",
    "pred = pred[0].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(y)\n",
    "axes[0].set_title('Ground Truth')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pred)\n",
    "axes[1].set_title('Predicted')\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
