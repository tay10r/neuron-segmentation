{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd537ec-23d3-4b4e-979b-171f586afcb6",
   "metadata": {},
   "source": [
    "<h1 style=\\\"text-align: center; font-size: 50px;\\\">üò∑ A tale of two cities analyzing trends </h1>\n",
    "This notebook shows an visual data analysis of the effects of COVID-19 in two different cities: New York and London"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d1d72",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Preparing the Data\n",
    "- Univariate Analysis\n",
    "- Bivariate Analysis\n",
    "- Correlation of features\n",
    "- Time-series Decomposition\n",
    "- Exponential Smoothing Forecasting Methods\n",
    "- Vector Autoregression (VAR) \n",
    "- Cointegration Test\n",
    "- Stationarity of a Time-Series\n",
    "- Training the VAR model\n",
    "- Autocorrelation of Residuals \n",
    "- Forecasting\n",
    "- Model Evaluation\n",
    "- Logging Model to MLflow\n",
    "- Fetching the Latest Model Version from MLflow\n",
    "- Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe7ff0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ed3c2",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 2.82276,
     "end_time": "2022-07-21T20:12:57.131891",
     "exception": false,
     "start_time": "2022-07-21T20:12:54.309131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------ Data Manipulation ------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------ Visualization Libraries ------------------------ \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ------------------------ System Utilities ------------------------\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ------------------------ Statistical and Time Series Analysis ------------------------\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning, ValueWarning\n",
    "\n",
    "# ------------------------ MLflow for Experiment Tracking and Model Management ------------------------\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, TensorSpec, ParamSchema, ParamSpec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727b12e",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot configurations\n",
    "plt.style.use('fivethirtyeight')\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'figure.figsize': (20,10),\n",
    "         'axes.labelsize': 25,\n",
    "         'xtick.labelsize':25,\n",
    "         'ytick.labelsize':25,\n",
    "        'font.family':'serif'}\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c492213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "warnings.simplefilter('ignore', ValueWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92575391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"cities_analysis_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\")  \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Paths -------------------------\n",
    "DATA_PATH = \"/home/jovyan/datafabric/tutorial/\"\n",
    "\n",
    "# ------------------------ MLflow Integration ------------------------\n",
    "EXPERIMENT_NAME = \"Two_Cities_Experiment\"\n",
    "RUN_NAME = \"Two_Cities_Run\"\n",
    "MODEL_NAME = \"Two_Cities_Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166ee00",
   "metadata": {},
   "source": [
    "## Verify Assets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e95e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the Dataset file exists\n",
    "is_dataset_available = Path(DATA_PATH).exists()\n",
    "\n",
    "# Log the configuration status of the dataset\n",
    "if is_dataset_available:\n",
    "    logger.info(\"The Dataset is properly configured.\")\n",
    "else:\n",
    "    logger.info(\n",
    "        \"The Dataset is not properly configured. Please create and download the required assets \"\n",
    "        \"in your project on AI Studio.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ac07a",
   "metadata": {
    "papermill": {
     "duration": 0.012023,
     "end_time": "2022-07-21T20:12:57.156282",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.144259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5acc7",
   "metadata": {
    "papermill": {
     "duration": 0.011898,
     "end_time": "2022-07-21T20:12:57.180282",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.168384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Acknowledgments:\n",
    "I'd like to thank the original authors of these data sources!\n",
    "\n",
    "| Data | Original Source |\n",
    "| --- | --- |\n",
    "| Mobility Data | [COVID-19 Community Mobility Reports](https://www.google.com/covid19/mobility/) |\n",
    "| NYC Cases | [NYC Department of Health and Mental Hygiene](https://www1.nyc.gov/site/doh/index.page) |\n",
    "| London Cases | [GOV.UK Coronavirus (COVID-19) in the UK](https://coronavirus.data.gov.uk/) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21de66f",
   "metadata": {
    "papermill": {
     "duration": 0.29149,
     "end_time": "2022-07-21T20:12:57.483924",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.192434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_folder = DATA_PATH\n",
    "ny_mobility = pd.read_csv(f\"{source_folder}/NewYork_mobility.csv\")\n",
    "ldn_mobility = pd.read_csv(f\"{source_folder}London_mobility.csv\")\n",
    "ny_cases = pd.read_csv(f\"{source_folder}daily_data_NewYork.csv\")\n",
    "ldn_cases = pd.read_csv(f\"{source_folder}daily_data_London.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b3534",
   "metadata": {
    "papermill": {
     "duration": 0.047635,
     "end_time": "2022-07-21T20:12:57.545403",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.497768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rename_mobility_cols(df):\n",
    "    \"\"\"\n",
    "    Renames the and cleans the dataset columns.\n",
    "\n",
    "    Parameters:\n",
    "        df(pd.DataFrame): A Dataframe containing the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned dataframe with the renamed columns.\n",
    "    \"\"\"\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'country_region':'country'})\n",
    "    df = df.rename(columns={'retail_and_recreation_percent_change_from_baseline':'retail'})\n",
    "    df = df.rename(columns={'grocery_and_pharmacy_percent_change_from_baseline':'pharmacy'})\n",
    "    df = df.rename(columns={'parks_percent_change_from_baseline':'parks'})\n",
    "    df = df.rename(columns={'transit_stations_percent_change_from_baseline':'transit_station'})\n",
    "    df = df.rename(columns={'workplaces_percent_change_from_baseline':'workplaces'})\n",
    "    df = df.rename(columns={'residential_percent_change_from_baseline':'residential'})\n",
    "    df.drop(['country_region_code','sub_region_1', 'sub_region_2', 'residential'], axis=1, inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "ny_mobility = ny_mobility.loc[ny_mobility['sub_region_2'] == \"New York County\"].reset_index(drop=True)\n",
    "ldn_mobility = ldn_mobility.loc[ldn_mobility['sub_region_2'] == \"City of London\"].reset_index(drop=True)\n",
    "\n",
    "ny_mobility = rename_mobility_cols(ny_mobility)\n",
    "ldn_mobility = rename_mobility_cols(ldn_mobility)\n",
    "\n",
    "mobility_features = ny_mobility.columns[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9347c6",
   "metadata": {
    "papermill": {
     "duration": 0.012002,
     "end_time": "2022-07-21T20:12:57.570115",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.558113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Mobility Features     | Description                                                                                                                           |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| country          | Country Name                                                                         |\n",
    "| metro_area       | Metropolitan area                                                                    |\n",
    "| iso_3166_2_code  | Codes for the names of the principal subdivisions (e.g. provinces or states)         |\n",
    "| census_fips_code | Census fips code                                                                     |\n",
    "| place_id         | Place IDs uniquely identify a place in the Google Places database and on Google Maps |\n",
    "| date             | Date                                                                                 |\n",
    "| retail          | Mobility trends for places like restaurants, cafes, shopping centers, theme parks, museums, libraries, and movie theaters.            |\n",
    "| pharmacy        | Mobility trends for places like grocery markets, food warehouses, farmers markets, specialty food shops, drug stores, and pharmacies. |\n",
    "| parks           | Mobility trends for places like local parks, national parks, public beaches, marinas, dog parks, plazas, and public gardens.          |\n",
    "| transit_station | Mobility trends for places like public transport hubs such as subway, bus, and train stations.                                        |\n",
    "| workplaces      | Mobility trends for places of work.                                                                                                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd209b4",
   "metadata": {
    "papermill": {
     "duration": 0.037799,
     "end_time": "2022-07-21T20:12:57.620000",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.582201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ldn_mobility.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4594a05",
   "metadata": {
    "papermill": {
     "duration": 0.012371,
     "end_time": "2022-07-21T20:12:57.645034",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.632663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Try it out yourself üöÄ : [Get the Address for a Place ID üåé](https://developers.google.com/maps/documentation/javascript/examples/geocoding-place-id)\n",
    "\n",
    "![](https://i.imgur.com/B69162k.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbb2b0",
   "metadata": {
    "papermill": {
     "duration": 0.025077,
     "end_time": "2022-07-21T20:12:57.682840",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.657763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rename_dailyData_cols(df):\n",
    "    \"\"\"\n",
    "    Renames the daily data DataFrame columns to standardized names.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A Dataframe containing daily COVID-19 statistics.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The Dataframe with the renamed columns.\n",
    "    \"\"\"\n",
    "    mapping = {df.columns[0]:'date', df.columns[1]: 'case_count', df.columns[2]:'hospitalized_count', df.columns[3]: 'death_count'} \n",
    "    df = df.rename(columns = mapping)\n",
    "    return df\n",
    "\n",
    "ny_cases = ny_cases[['date_of_interest','CASE_COUNT','HOSPITALIZED_COUNT','DEATH_COUNT']]\n",
    "ldn_cases = ldn_cases[['date','newCasesBySpecimenDate', 'newAdmissions', 'newDeaths28DaysByDeathDate']]\n",
    "\n",
    "ny_cases = rename_dailyData_cols(ny_cases)\n",
    "ldn_cases = rename_dailyData_cols(ldn_cases)\n",
    "\n",
    "cases_features = ny_cases.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7308a",
   "metadata": {
    "papermill": {
     "duration": 0.012104,
     "end_time": "2022-07-21T20:12:57.707548",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.695444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Cases Features     | Description                    |\n",
    "|--------------------|--------------------------------|\n",
    "| date               | Date                           |\n",
    "| case_count         | Number of daily cases recorded |\n",
    "| hospitalized_count | Number of people hospitalized  |\n",
    "| death_count        | Number of deaths recorded      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a94c84",
   "metadata": {
    "papermill": {
     "duration": 0.025326,
     "end_time": "2022-07-21T20:12:57.745203",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.719877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ny_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7633a",
   "metadata": {
    "papermill": {
     "duration": 0.027912,
     "end_time": "2022-07-21T20:12:57.785816",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.757904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for df in ny_mobility, ldn_mobility, ny_cases, ldn_cases:\n",
    "    df['date'] = df['date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9154c6",
   "metadata": {
    "papermill": {
     "duration": 0.03623,
     "end_time": "2022-07-21T20:12:57.834731",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.798501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_data(mobility, cases):\n",
    "    \"\"\"\n",
    "    Merges mobility and case data on the 'date' column using an inner join.\n",
    "\n",
    "    Parameters:\n",
    "        mobility (pd.DataFrame): DataFrame containing mobility indicators.\n",
    "        cases (pd.DataFrame): DataFrame containing COVID-19 case statistics.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame containing both mobility and case data, aligned by date.\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(mobility, cases, how='inner', on = 'date')\n",
    "    return merged_df\n",
    "\n",
    "# setting date as the index column\n",
    "ny_df = merge_data(ny_mobility, ny_cases).set_index('date')\n",
    "ldn_df = merge_data(ldn_mobility, ldn_cases).set_index('date')\n",
    "ldn_df = ldn_df.iloc[:-2,:]\n",
    "\n",
    "features = ny_df.columns[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ed2c7",
   "metadata": {
    "papermill": {
     "duration": 0.03355,
     "end_time": "2022-07-21T20:12:57.880974",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.847424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ny_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f9651",
   "metadata": {
    "papermill": {
     "duration": 0.033316,
     "end_time": "2022-07-21T20:12:57.927480",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.894164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ldn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7883067",
   "metadata": {
    "papermill": {
     "duration": 0.023026,
     "end_time": "2022-07-21T20:12:57.963893",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.940867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# during and after the pandemic dates \n",
    "start_date = min(ny_df.index.min(), ldn_df.index.min()).strftime('%Y-%m-%d')\n",
    "split_date = '2022-02-01'\n",
    "end_date = max(ny_df.index.max(), ldn_df.index.max()).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d747dc8",
   "metadata": {
    "papermill": {
     "duration": 0.012869,
     "end_time": "2022-07-21T20:12:57.990165",
     "exception": false,
     "start_time": "2022-07-21T20:12:57.977296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbadbaa",
   "metadata": {
    "papermill": {
     "duration": 0.023904,
     "end_time": "2022-07-21T20:12:58.027175",
     "exception": false,
     "start_time": "2022-07-21T20:12:58.003271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def univariate_trends(idx): \n",
    "    \"\"\"\n",
    "    Plots the time series trend of a single mobility or case feature for both New York and London.\n",
    "\n",
    "    Parameters:\n",
    "        idx (int): Index of the feature to plot, based on the 'features' list.\n",
    "    \"\"\"\n",
    "    plt.rcParams.update(params)          \n",
    "    ax = sns.lineplot(x=ny_df.index, y=features[idx], data=ny_df)\n",
    "    ax = sns.lineplot(x=ldn_df.index, y=features[idx], data=ldn_df)\n",
    "    \n",
    "    text_pos_x1 = ax.get_xlim()[0] + 300\n",
    "    text_pos_x2 = ax.get_xlim()[1] - 175\n",
    "    text_pos_y = ax.get_ylim()[1]\n",
    "    plt.text(x=text_pos_x1, y=text_pos_y, s='COVID era', alpha=0.7, color='#334f8d', size = 25, weight='bold')\n",
    "    plt.text(x=text_pos_x2, y=text_pos_y, s='Post COVID era', alpha=0.7, color='#334f8d', size = 25, weight='bold')\n",
    "    \n",
    "    plt.axvspan(start_date, split_date, color='y', alpha=0.1, lw=0)\n",
    "    plt.axvspan(split_date, end_date, color='g', alpha=0.1, lw=0)\n",
    "    \n",
    "    plt.suptitle(\"Change in \" + features[idx] + \" over time\", size=30, color='#334f8d', weight=\"bold\")\n",
    "    \n",
    "    plt.legend(labels=['New York', 'London'], bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed795e3",
   "metadata": {
    "papermill": {
     "duration": 4.088483,
     "end_time": "2022-07-21T20:13:02.128720",
     "exception": false,
     "start_time": "2022-07-21T20:12:58.040237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trends over time\n",
    "for idx in range(len(features)):\n",
    "    univariate_trends(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa6f87",
   "metadata": {
    "papermill": {
     "duration": 0.041827,
     "end_time": "2022-07-21T20:13:02.213823",
     "exception": false,
     "start_time": "2022-07-21T20:13:02.171996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fed803",
   "metadata": {
    "papermill": {
     "duration": 0.057767,
     "end_time": "2022-07-21T20:13:02.314256",
     "exception": false,
     "start_time": "2022-07-21T20:13:02.256489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bivariate_trends(df, idx1, idx2, place):\n",
    "    \"\"\"\n",
    "    Plots a dual-axis time series chart showing the relationship between two features over time for a given location.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame for the selected city.\n",
    "        idx1 (int): Index of the first feature to plot.\n",
    "        idx2 (int): Index of the second feature to plot.\n",
    "        place (str): Name of the city/region.\n",
    "    \"\"\"\n",
    "    colors = [\"#ff9e00\", \"#a6808c\",\"#70a38d\", \"#6665dd\", \"#b57ba6\"]\n",
    "    color1 = colors[idx1]\n",
    "    if place=='New York': \n",
    "        color2 = \"#30a2da\" \n",
    "    else: \n",
    "        color2 = \"#fc4f30\"\n",
    "         \n",
    "    plt.rcParams.update(params)\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    sns.lineplot(x=df.index, y=features[idx1], data=df, color=color1, ax=ax)\n",
    "    ax.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    sns.lineplot(x=df.index, y=features[idx2], data=df, color=color2, ax=ax2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    \n",
    "    text_pos_x1 = ax.get_xlim()[0] + 300\n",
    "    text_pos_x2 = ax.get_xlim()[1] - 175\n",
    "    text_pos_y = ax2.get_ylim()[1]\n",
    "    plt.text(x=text_pos_x1, y=text_pos_y, s='COVID era', alpha=0.7, color='#334f8d', size = 25, weight='bold')\n",
    "    plt.text(x=text_pos_x2, y=text_pos_y, s='Post COVID era', alpha=0.7, color='#334f8d', size = 25, weight='bold')\n",
    "    \n",
    "    plt.axvspan(start_date, split_date, color='y', alpha=0.1, lw=0)\n",
    "    plt.axvspan(split_date, end_date, color='g', alpha=0.1, lw=0)\n",
    "    \n",
    "    plt.legend(labels = [place,features[idx1]], bbox_to_anchor=(1, 1.2), loc='upper left', borderaxespad=0, facecolor=\"white\")\n",
    "    leg = ax2.get_legend()\n",
    "    leg.legendHandles[1].set_color(color1)\n",
    "    leg.legendHandles[1].set_alpha(1)\n",
    "    \n",
    "    plt.suptitle(place + \" : Change in \" + features[idx1] + \" and \"  + features[idx2] + \" over time\", size=30, color='#334f8d', weight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c49d3",
   "metadata": {
    "papermill": {
     "duration": 5.096295,
     "end_time": "2022-07-21T20:13:07.453073",
     "exception": false,
     "start_time": "2022-07-21T20:13:02.356778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(len(mobility_features)):\n",
    "    bivariate_trends(ny_df,idx, 5, \"New York\")\n",
    "    bivariate_trends(ldn_df,idx, 5, \"London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ed89a",
   "metadata": {
    "papermill": {
     "duration": 0.090393,
     "end_time": "2022-07-21T20:13:07.631006",
     "exception": false,
     "start_time": "2022-07-21T20:13:07.540613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Correlation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ea977",
   "metadata": {
    "papermill": {
     "duration": 0.102071,
     "end_time": "2022-07-21T20:13:07.820028",
     "exception": false,
     "start_time": "2022-07-21T20:13:07.717957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_correlation(df, place):\n",
    "    \"\"\"\n",
    "    Plots side-by-side heatmaps of the correlation matrices of selected features for two\n",
    "    different time periods: during COVID and post-COVID.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with time-indexed features for a specific region.\n",
    "        place (str): Name of the city/region to be used in the plot title.\n",
    "    \"\"\"\n",
    "    plt.rcParams.update(params)\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    df_during = df.loc[df.index < split_date]\n",
    "    df_post = df.loc[df.index >= split_date]\n",
    "    \n",
    "    corr1=df_during[features].corr()\n",
    "    corr2=df_post[features].corr()\n",
    "    \n",
    "    mask1 = np.triu(np.ones_like(corr1, dtype=bool))\n",
    "    mask2 = np.triu(np.ones_like(corr2, dtype=bool))\n",
    "    \n",
    "    fig.suptitle(\"Correlation of features in \" + place, fontsize = 30)\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.heatmap(corr1, mask=mask1, cmap='BuPu', \n",
    "                square=True, linewidths=.5,annot=True, fmt='.2f', cbar=False, ax=ax[0])\n",
    "    sns.heatmap(corr2, mask=mask2, cmap='BuPu', \n",
    "                square=True, linewidths=.5,annot=True, fmt='.2f', cbar=False, ax=ax[1])\n",
    "    \n",
    "    ax[0].title.set_text('COVID era')\n",
    "    ax[1].title.set_text('Post COVID era')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47099f18",
   "metadata": {
    "papermill": {
     "duration": 1.726483,
     "end_time": "2022-07-21T20:13:09.632998",
     "exception": false,
     "start_time": "2022-07-21T20:13:07.906515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#correlation of features\n",
    "plot_correlation(ny_df, \"New York\")\n",
    "plot_correlation(ldn_df, \"London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c36a5",
   "metadata": {
    "papermill": {
     "duration": 0.091914,
     "end_time": "2022-07-21T20:13:09.817923",
     "exception": false,
     "start_time": "2022-07-21T20:13:09.726009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "üìå **Mobility Trend Attributes**\n",
    "- **COVID era**: Strong pair correlations indicating that the global lockdown restrictions resulted in a drop in visits in all areas.\n",
    "- **Post COVID era**: Fall in pair correlations indicating that the trends are changing.\n",
    "\n",
    "üìå **Cases Attributes**\n",
    "- **COVID era**: Strong pair correlations between death count and hospitalized count.\n",
    "- **Post COVID era**: Fall in pair correlation of death count and hospitalized count indicating that the trends are changing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4dd8ae",
   "metadata": {
    "papermill": {
     "duration": 0.094185,
     "end_time": "2022-07-21T20:13:10.006596",
     "exception": false,
     "start_time": "2022-07-21T20:13:09.912411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Time-series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb1a53",
   "metadata": {
    "papermill": {
     "duration": 0.091938,
     "end_time": "2022-07-21T20:13:10.192913",
     "exception": false,
     "start_time": "2022-07-21T20:13:10.100975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Time series Decomposition helps us to analyze data as a combination of level, trend, seasonality and noise components.\n",
    "\n",
    "How does this help us? ü§®\n",
    "- Allows us to focus on predicting the general trend of the data üìà\n",
    "- Reveals note-worthy behavior in the seasonal component ‚òÄÔ∏èüçÇ‚ùÑÔ∏èüå±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180bfd5",
   "metadata": {
    "papermill": {
     "duration": 0.103196,
     "end_time": "2022-07-21T20:13:10.388532",
     "exception": false,
     "start_time": "2022-07-21T20:13:10.285336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_series_plot(df, col, period):\n",
    "    \"\"\"\n",
    "    Performs and visualizes seasonal decomposition of time series into trend,\n",
    "    seasonal and residual components.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Time-indexed DataFrame containing the time siries.\n",
    "        col (str): Name of the column to decompose.\n",
    "        period (int): Number of the observations per cycle.\n",
    "    \"\"\"\n",
    "    decomposition = seasonal_decompose(df[col], period = period)\n",
    "    \n",
    "    plt.rcParams.update(params)\n",
    "    figure = decomposition.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f3510",
   "metadata": {
    "papermill": {
     "duration": 2.599733,
     "end_time": "2022-07-21T20:13:13.080820",
     "exception": false,
     "start_time": "2022-07-21T20:13:10.481087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'New York:' + color.END)\n",
    "time_series_plot(ny_df, cases_features[0],12)\n",
    "time_series_plot(ny_df, cases_features[0],52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf121e55",
   "metadata": {
    "papermill": {
     "duration": 0.107285,
     "end_time": "2022-07-21T20:13:13.290070",
     "exception": false,
     "start_time": "2022-07-21T20:13:13.182785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "    def __init__(self, embed_str=None):\n",
    "        self.embed_str = embed_str\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.embed_str\n",
    "\n",
    "s = (\"\"\"\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">New York state reports highest number of daily Covid cases of entire pandemic at more than 21,000 <a href=\"https://t.co/0VIqv8lu5G\">https://t.co/0VIqv8lu5G</a></p>&mdash; CNBC (@CNBC) <a href=\"https://twitter.com/CNBC/status/1471945248419074055?ref_src=twsrc%5Etfw\">December 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\"\"\")\n",
    "\n",
    "Tweet(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e908226",
   "metadata": {
    "papermill": {
     "duration": 2.852545,
     "end_time": "2022-07-21T20:13:16.241998",
     "exception": false,
     "start_time": "2022-07-21T20:13:13.389453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'London:' + color.END)\n",
    "time_series_plot(ldn_df, cases_features[0],12)\n",
    "time_series_plot(ldn_df, cases_features[0],52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4e5e6",
   "metadata": {
    "papermill": {
     "duration": 0.10775,
     "end_time": "2022-07-21T20:13:16.458736",
     "exception": false,
     "start_time": "2022-07-21T20:13:16.350986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exponential Smoothing Forecasting Methods\n",
    "\n",
    "> A family of forecasting models focused on utilizing **weighted averages of past observations to forecast new values**. \n",
    ">\n",
    "> These methods are also called ETS models, referring to the explicit modeling of **Error, Trend and Seasonality**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c4bd1",
   "metadata": {
    "papermill": {
     "duration": 0.123532,
     "end_time": "2022-07-21T20:13:16.694401",
     "exception": false,
     "start_time": "2022-07-21T20:13:16.570869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_exp_smoothing(df, col, triple=False):\n",
    "    \"\"\"\n",
    "    Applies Simple Exponential Smoothing (SES) to a time series and plots the original data,\n",
    "    the fitted values, and a 52-period forecast.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Time-indexed DataFrame with the target series.\n",
    "        col (str): Name of the column.\n",
    "        triple (bool): If 'True', aplies SES with three smoothing levels (alpha = 0.2, 0.5, 0.9) for comparison.\n",
    "                       If 'False', uses only alpha = 0.5.\n",
    "    \"\"\"\n",
    "    df = df[[col]]\n",
    "    plt.rcParams.update(params)\n",
    "    if triple == True:\n",
    "        for alpha_sm, color in zip([0.2, 0.5, 0.9], ['red', 'orange', 'brown']):\n",
    "\n",
    "            df.plot.line()\n",
    "\n",
    "            fit1 = SimpleExpSmoothing(df).fit(smoothing_level = alpha_sm  ,optimized=False)\n",
    "            fcast1 = fit1.forecast(52).rename('alpha = ' + str(alpha_sm))\n",
    "            fcast1.plot(marker='o', color=color, legend=True)\n",
    "            fit1.fittedvalues.plot(color=color)\n",
    "\n",
    "            plt.show()\n",
    "    else:\n",
    "            df.plot.line()\n",
    "            color = 'orange'\n",
    "            fit1 = SimpleExpSmoothing(df).fit(smoothing_level = 0.5  ,optimized=False)\n",
    "            fcast1 = fit1.forecast(52).rename('alpha = 0.5')\n",
    "            fcast1.plot(marker='o', color=color, legend=True)\n",
    "            fit1.fittedvalues.plot(color=color)\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "def double_exp_smoothing(df, col):\n",
    "    \"\"\"\n",
    "    Applies Double Exponential Smoothing to capture level and trend in the time series, and plots the original data,\n",
    "    the fitted values, and a 52-period forecast.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Time-indexed DataFrame with the target series.\n",
    "        col (str): Name of the column.\n",
    "    \"\"\"\n",
    "    df = df[[col]]\n",
    "    plt.rcParams.update(params)\n",
    "    \n",
    "    df.plot.line()\n",
    "    fit1 = Holt(df).fit(smoothing_level=0.5, smoothing_slope=0.5, optimized=False)\n",
    "    fcast1 = fit1.forecast(52).rename(\"Double Exponential Smoothing\")\n",
    "    fit1.fittedvalues.plot(color='orange')\n",
    "    fcast1.plot(color='orange', legend=True)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def triple_exp_smoothing(df, col):\n",
    "    \"\"\"\n",
    "    Applies Triple Exponential Smoothing to model trend and seasonality in time series, and plots the original data,\n",
    "    the fitted values, and a 52-period forecast.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Time-indexed DataFrame with the target series.\n",
    "        col (str): Name of the column.\n",
    "    \"\"\"\n",
    "    df = df[[col]]\n",
    "    plt.rcParams.update(params)\n",
    "    \n",
    "    df.plot.line()\n",
    "    fit1 = ExponentialSmoothing(df, seasonal_periods=12, trend='add', seasonal='add')\n",
    "    fit1 = fit1.fit(smoothing_level=0.5)\n",
    "    fit1.fittedvalues.plot(color='orange')\n",
    "    fit1.forecast(52).rename(\"Triple Exponential Smoothing\").plot(color='red', legend=True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986eeb3",
   "metadata": {
    "papermill": {
     "duration": 0.108382,
     "end_time": "2022-07-21T20:13:16.914016",
     "exception": false,
     "start_time": "2022-07-21T20:13:16.805634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Simple Exponential Smoothing\n",
    "\\begin{equation}\n",
    "\\hat  Y_{t+1} = \\hat Y_t + \\alpha  (Y_t - \\hat Y_t)\n",
    "\\end{equation}\n",
    "\n",
    "- Predicted value for time period t+1 = Predicted value for previous time period + Adjustment for the error made in predicting the previous period's value\n",
    "\n",
    "- Œ± can assume any value between 0 and 1.\n",
    "\n",
    "- For small values of Œ±, estimates change very slowly.\n",
    "\n",
    "- For large values of Œ±, estimates change rapidly, and our prediction is basically the most recent observation.\n",
    "\n",
    "- As we move towards forecasting the next observed value and we run out of data points, we notice that the equation takes the form of:\n",
    "\\begin{equation}\n",
    "\\hat  Y_{t+1} = \\hat Y_t \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069df5db",
   "metadata": {
    "papermill": {
     "duration": 1.491562,
     "end_time": "2022-07-21T20:13:18.513616",
     "exception": false,
     "start_time": "2022-07-21T20:13:17.022054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_exp_smoothing(ny_df, \"case_count\", triple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04723ab6",
   "metadata": {
    "papermill": {
     "duration": 0.115904,
     "end_time": "2022-07-21T20:13:18.746247",
     "exception": false,
     "start_time": "2022-07-21T20:13:18.630343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 2. Double Exponential Smoothing (Holt's method)\n",
    "\n",
    "Forecasting function:\n",
    "\\begin{equation}\n",
    "\\hat  Y_{t+n} = E_t + nT_t\n",
    "\\end{equation}\n",
    "\n",
    "Computing the base level for time period t\n",
    "\\begin{equation}\n",
    "E_t = \\alpha Y_t + (1-\\alpha)(E_{t-1} + T_{t-1})\n",
    "\\end{equation}\n",
    "\n",
    "Computing the expected trend value for time period t\n",
    "\\begin{equation}\n",
    "T_t = \\beta (E_t - E_{t-1}) + (1-\\beta)T_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "- 0 <= Œ± <= 1 and 0 <= Œ≤ <= 1\n",
    "\n",
    "Ideal for time series data with a linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19381409",
   "metadata": {
    "papermill": {
     "duration": 0.599015,
     "end_time": "2022-07-21T20:13:19.466535",
     "exception": false,
     "start_time": "2022-07-21T20:13:18.867520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "double_exp_smoothing(ny_df, \"case_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d15f70",
   "metadata": {
    "papermill": {
     "duration": 0.114419,
     "end_time": "2022-07-21T20:13:19.700551",
     "exception": false,
     "start_time": "2022-07-21T20:13:19.586132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Triple Exponential Smoothing (Holt-Winter's method)\n",
    "\n",
    "\n",
    "Forecasting function:\n",
    "\\begin{equation}\n",
    "\\hat  Y_{t+n} = E_t + nT_t + S_{t+n-p}\n",
    "\\end{equation}\n",
    "\n",
    "Computing the base level for time period t\n",
    "\\begin{equation}\n",
    "E_t = \\alpha (Y_t-S_{t-p}) + (1-\\alpha)(E_{t-1} + T_{t-1})\n",
    "\\end{equation}\n",
    "\n",
    "Computing the expected trend value for time period t\n",
    "\\begin{equation}\n",
    "T_t = \\beta (E_t - E_{t-1}) + (1-\\beta)T_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "Computing the expected seasonal factor for time period t\n",
    "\\begin{equation}\n",
    "S_t = \\gamma (Y_t - E_t) + (1-\\gamma)S_{t-p}\n",
    "\\end{equation}\n",
    "\n",
    "- 0 <= Œ± <= 1 , 0 <= Œ≤ <= 1 and 0 <= Œ≥ <= 1\n",
    "- p represents the number of seasons in the time series\n",
    "\n",
    "Applicable for time series data exhibiting a trend and seasonality component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f600574",
   "metadata": {
    "papermill": {
     "duration": 0.647991,
     "end_time": "2022-07-21T20:13:20.465916",
     "exception": false,
     "start_time": "2022-07-21T20:13:19.817925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'New York:' + color.END)\n",
    "triple_exp_smoothing(ny_df, \"case_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af6fd6",
   "metadata": {
    "papermill": {
     "duration": 0.627195,
     "end_time": "2022-07-21T20:13:21.211266",
     "exception": false,
     "start_time": "2022-07-21T20:13:20.584071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'London:' + color.END)\n",
    "triple_exp_smoothing(ldn_df, \"case_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ccd80",
   "metadata": {
    "papermill": {
     "duration": 0.123461,
     "end_time": "2022-07-21T20:13:21.453251",
     "exception": false,
     "start_time": "2022-07-21T20:13:21.329790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vector Autoregression (VAR) \n",
    "\n",
    "Multivariate forecasting algorithm that can be used when **two or more time series influence each other** ‚û°Ô∏è‚¨ÖÔ∏è \n",
    "\n",
    "**Advantages**\n",
    "- Ability to capture sophisticated real-world behaviour and interconnected dynamics of time series data ‚úÖ \n",
    "- Improved forecasting efficiency ‚úÖ  \n",
    "\n",
    "**Procedure**\n",
    "\n",
    "Each variable is modeled as a **linear combination** of: \n",
    "- its own past values\n",
    "- the past values of other variables in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d9e7b",
   "metadata": {
    "papermill": {
     "duration": 0.120942,
     "end_time": "2022-07-21T20:13:21.696810",
     "exception": false,
     "start_time": "2022-07-21T20:13:21.575868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cointegration Test\n",
    "\n",
    "**Note**\n",
    "\n",
    ">  ‚ö†Ô∏è Cointegration is not the same as correlation ‚ö†Ô∏è\n",
    "> \n",
    "> - Correlation measures whether two or more time-series variables move together in the long-run.\n",
    "> - Cointegration measures whether the difference between their means remains constant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc65a3d",
   "metadata": {
    "papermill": {
     "duration": 0.135704,
     "end_time": "2022-07-21T20:13:21.952924",
     "exception": false,
     "start_time": "2022-07-21T20:13:21.817220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ny_data = ny_df.iloc[:,5:]\n",
    "ldn_data = ldn_df.iloc[:,5:].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95736546",
   "metadata": {
    "papermill": {
     "duration": 0.121623,
     "end_time": "2022-07-21T20:13:22.195300",
     "exception": false,
     "start_time": "2022-07-21T20:13:22.073677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Johansen's Test**\n",
    "\n",
    "*To check if three or more time series are cointegrated*\n",
    "- Null hypothesis: the time series are not cointegrated\n",
    "- If the trace statistic is greater than the critical value, we reject the null hypothesis and accept the alternate hypothesis, suggesting that the series are cointegrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb009e60",
   "metadata": {
    "papermill": {
     "duration": 0.153164,
     "end_time": "2022-07-21T20:13:22.470140",
     "exception": false,
     "start_time": "2022-07-21T20:13:22.316976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def johansens_test(df, alpha=0.05): \n",
    "    \"\"\"\n",
    "    Performs Johansen's cointegration test on a set of time series and print the results\n",
    "    including trace statistics, critical values, and whether the null hypothesis is rejected.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing multiple time series.\n",
    "        alpha (float): Significance level for the test. Defaults to 0.05.\n",
    "    \"\"\"\n",
    "    result = coint_johansen(df,-1,1)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = result.lr1\n",
    "    cvts = result.cvt[:, d[str(1-alpha)]]\n",
    "    \n",
    "    print(\"{: >20} {: >20} {: >20} {: >10}\".format(*['Column name', 'Trace Statistic' , 'Critical Value(95%)', 'Null Hypothesis Rejected' ]))\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        ans = \"Yes\" if trace > cvt else \"No\"\n",
    "        print(\"{: >20} {: >20} {: >20} {: >10}\".format(*[col, round(trace,2) ,cvt, ans]))\n",
    "\n",
    "def cointegration_test(data):\n",
    "    \"\"\"\n",
    "    Splits a time series DataFrame into two periods and \n",
    "    performs Johansen's cointegration test on each subset, printing the results\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame with a time-index and multiple time series columns.\n",
    "    \"\"\"\n",
    "    data_during = data.loc[data.index < split_date]\n",
    "    data_post = data.loc[data.index >= split_date]\n",
    "\n",
    "    print(color.BOLD + color.DARKCYAN + 'During the pandemic:' + color.END)\n",
    "    johansens_test(data_during)\n",
    "    print(\"\\n\\n\"+color.BOLD + color.DARKCYAN + 'Post pandemic:' + color.END)\n",
    "    johansens_test(data_post)\n",
    "\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'New York:' + color.END)\n",
    "cointegration_test(ny_data)\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'London:' + color.END)\n",
    "cointegration_test(ldn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e208e6",
   "metadata": {
    "papermill": {
     "duration": 0.117486,
     "end_time": "2022-07-21T20:13:22.765850",
     "exception": false,
     "start_time": "2022-07-21T20:13:22.648364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stationarity of a Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bc5ca",
   "metadata": {
    "papermill": {
     "duration": 0.119476,
     "end_time": "2022-07-21T20:13:23.006387",
     "exception": false,
     "start_time": "2022-07-21T20:13:22.886911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before we apply the VAR model we need to ensure that all the time series variables in the data are stationary.\n",
    "\n",
    "**Augmented Dickey-Fuller Test**\n",
    "\n",
    "*To check if all the time series variables in the data are stationary*\n",
    "- Null hypothesis: the time series is considered non-stationary\n",
    "- If the p-value of the ADF test is less than the significance level then we reject the null hypothesis and accept the alternate hypothesis, considering that the time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d0b8b",
   "metadata": {
    "papermill": {
     "duration": 0.129739,
     "end_time": "2022-07-21T20:13:23.257915",
     "exception": false,
     "start_time": "2022-07-21T20:13:23.128176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adfuller_test(series, series_name):\n",
    "    \"\"\"\n",
    "    Performs the Augmented Dickey-Fuller (ADF) Test to check for stationarity in a time series.\n",
    "    Prints the p-value, significance level, and wether the null hypothesis (non-stationarity) is rejected.\n",
    "\n",
    "    Parameters:\n",
    "        series (pd.Series): The time series data to be tested.\n",
    "        series_name (str): Name of the series, used for display purposes in the output\n",
    "    \"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    result = {'ADF_test_statistic':round(r[0], 4), 'p-value':round(r[1], 4), 'num_lags':round(r[2], 4), 'num_observations':r[3]}\n",
    "    \n",
    "    significance_lvl=0.05\n",
    "    p_value = result['p-value'] \n",
    "    \n",
    "    ans = \"Yes\" if p_value <= significance_lvl else \"No\" \n",
    "    \n",
    "    print(\"{:>18} {: >10} {: >15} {: >25} {: >10}\".format(*[series_name, p_value, significance_lvl,  ans, ans ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282a32e",
   "metadata": {
    "papermill": {
     "duration": 0.129515,
     "end_time": "2022-07-21T20:13:23.509566",
     "exception": false,
     "start_time": "2022-07-21T20:13:23.380051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_obs = 7\n",
    "ny_train, ny_test = ny_data[0:-num_obs], ny_data[-num_obs:]\n",
    "ldn_train, ldn_test = ldn_data[0:-num_obs], ldn_data[-num_obs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d0d52",
   "metadata": {
    "papermill": {
     "duration": 1.046213,
     "end_time": "2022-07-21T20:13:24.677985",
     "exception": false,
     "start_time": "2022-07-21T20:13:23.631772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_ADF(df):\n",
    "    \"\"\"\n",
    "    Applies the Augmented Dickey-Fuller (ADF) Test to each time series in the DataFrame\n",
    "    and prints a summary table showing the p-value, significance level, and wether the series is stationary.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame where each column is a univariate time series to be tested.\n",
    "    \"\"\"\n",
    "    print(\"{:>18} {: >10} {: >15} {: >25} {: >10}\".format(*['Series', 'P-Value' ,'Significance Level', 'Null Hypothesis Rejected', 'Stationary' ]))\n",
    "    for col_name, col_data in df.items():\n",
    "        adfuller_test(col_data, col_name)\n",
    "\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'New York:' + color.END)\n",
    "display_ADF(ny_train)\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'London:' + color.END)\n",
    "display_ADF(ldn_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191809d9",
   "metadata": {
    "papermill": {
     "duration": 0.118221,
     "end_time": "2022-07-21T20:13:24.975873",
     "exception": false,
     "start_time": "2022-07-21T20:13:24.857652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Some series aren't stationary so we will be taking the first-order difference of the entire data.\n",
    "\n",
    "- After this we will re-run the ADF test on each differenced series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59400d9",
   "metadata": {
    "papermill": {
     "duration": 0.588944,
     "end_time": "2022-07-21T20:13:25.685951",
     "exception": false,
     "start_time": "2022-07-21T20:13:25.097007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ny_differenced = ny_train.diff().dropna()\n",
    "ldn_differenced = ldn_train.diff().dropna()\n",
    "\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'New York:' + color.END)\n",
    "display_ADF(ny_differenced)\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'London:' + color.END)\n",
    "display_ADF(ldn_differenced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e93d97",
   "metadata": {
    "papermill": {
     "duration": 0.119188,
     "end_time": "2022-07-21T20:13:25.979650",
     "exception": false,
     "start_time": "2022-07-21T20:13:25.860462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the series are now stationary!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bb2e2",
   "metadata": {
    "papermill": {
     "duration": 0.119687,
     "end_time": "2022-07-21T20:13:26.221723",
     "exception": false,
     "start_time": "2022-07-21T20:13:26.102036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the VAR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb1a2f",
   "metadata": {
    "papermill": {
     "duration": 0.122385,
     "end_time": "2022-07-21T20:13:26.464254",
     "exception": false,
     "start_time": "2022-07-21T20:13:26.341869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We select the order that gives a model with least BIC.\n",
    "\n",
    "> BIC or Bayesian Information Criterion is a method for scoring and selecting a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845291a",
   "metadata": {
    "papermill": {
     "duration": 0.277368,
     "end_time": "2022-07-21T20:13:26.863454",
     "exception": false,
     "start_time": "2022-07-21T20:13:26.586086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ny_model = VAR(ny_differenced)\n",
    "res = ny_model.select_order(maxlags=15)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c0495",
   "metadata": {
    "papermill": {
     "duration": 0.238634,
     "end_time": "2022-07-21T20:13:27.273995",
     "exception": false,
     "start_time": "2022-07-21T20:13:27.035361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ldn_model = VAR(ldn_differenced)\n",
    "res = ldn_model.select_order(maxlags=15)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8896d7",
   "metadata": {
    "papermill": {
     "duration": 0.119109,
     "end_time": "2022-07-21T20:13:27.565933",
     "exception": false,
     "start_time": "2022-07-21T20:13:27.446824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ccb32",
   "metadata": {
    "papermill": {
     "duration": 0.151553,
     "end_time": "2022-07-21T20:13:27.838033",
     "exception": false,
     "start_time": "2022-07-21T20:13:27.686480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ny_model = ny_model.fit(7)\n",
    "ldn_model = ldn_model.fit(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d960a7",
   "metadata": {
    "papermill": {
     "duration": 0.121781,
     "end_time": "2022-07-21T20:13:28.137502",
     "exception": false,
     "start_time": "2022-07-21T20:13:28.015721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Autocorrelation of Residuals \n",
    "\n",
    "Durbin Watson‚Äôs Statistic\n",
    "\n",
    "*To check for autocorrelation in a regression model's output*\n",
    "- Values range from 0 to 4, with a value of 2 indicating **zero autocorrelation**.\n",
    "- Values below 2 indicate **positive autocorrelation**. \n",
    "- Values above 2 indicate **negative autocorrelation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd3bc0",
   "metadata": {
    "papermill": {
     "duration": 0.139446,
     "end_time": "2022-07-21T20:13:28.399198",
     "exception": false,
     "start_time": "2022-07-21T20:13:28.259752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DW(model, data):\n",
    "    \"\"\"\n",
    "    Computes the Durbin Watson‚Äôs Statistic for residuals of a fitted time series model,\n",
    "    assessing the presence of autocorrelation for each series in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        model: Fitted VAR or similar multivariate time series model with a '.resid' attribute.\n",
    "        data (pd.DataFrame): Original time series data used in the model, used for column labels.\n",
    "    \"\"\"\n",
    "    result = durbin_watson(model.resid)\n",
    "\n",
    "    print(\"{:>18} {: >15}\".format(*['Series', 'DW Statistic']))\n",
    "    for col, val in zip(data.columns, result):\n",
    "        print(\"{:>18} {: >15}\".format(*[col, round(val, 2)]))\n",
    "        \n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'New York:' + color.END)\n",
    "DW(ny_model, ny_data)\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'London:' + color.END)\n",
    "DW(ldn_model, ldn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f122c02",
   "metadata": {
    "papermill": {
     "duration": 0.11713,
     "end_time": "2022-07-21T20:13:28.691997",
     "exception": false,
     "start_time": "2022-07-21T20:13:28.574867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- In the event that auto correlation exists, it undervalues the standard error and may cause us to believe that predictors are significant when in reality they are not.\n",
    "\n",
    "- Since the values are closer to 2 in our case, we can safely proceed with the VAR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6c006",
   "metadata": {
    "papermill": {
     "duration": 0.119954,
     "end_time": "2022-07-21T20:13:29.293392",
     "exception": false,
     "start_time": "2022-07-21T20:13:29.173438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2fe57",
   "metadata": {
    "papermill": {
     "duration": 0.142899,
     "end_time": "2022-07-21T20:13:29.556862",
     "exception": false,
     "start_time": "2022-07-21T20:13:29.413963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lag_order = ny_model.k_ar\n",
    "ny_forecast_input = ny_differenced.values[-lag_order:]\n",
    "ny_differenced[-lag_order:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e8d53",
   "metadata": {
    "papermill": {
     "duration": 0.138018,
     "end_time": "2022-07-21T20:13:29.815315",
     "exception": false,
     "start_time": "2022-07-21T20:13:29.677297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lag_order = ldn_model.k_ar\n",
    "ldn_forecast_input = ldn_differenced.values[-lag_order:]\n",
    "ldn_differenced[-lag_order:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125d20d",
   "metadata": {
    "papermill": {
     "duration": 0.138442,
     "end_time": "2022-07-21T20:13:30.074552",
     "exception": false,
     "start_time": "2022-07-21T20:13:29.936110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fc = ny_model.forecast(y=ny_forecast_input, steps=num_obs)\n",
    "ny_forecast_output = pd.DataFrame(fc, index=ny_data.index[-num_obs:], columns=ny_data.columns + '_forecast')\n",
    "ny_forecast_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b59bd",
   "metadata": {
    "papermill": {
     "duration": 0.129157,
     "end_time": "2022-07-21T20:13:30.323618",
     "exception": false,
     "start_time": "2022-07-21T20:13:30.194461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fc = ldn_model.forecast(y=ldn_forecast_input, steps=num_obs)\n",
    "ldn_forecast_output = pd.DataFrame(fc, index=ldn_data.index[-num_obs:], columns=ldn_data.columns + '_forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d889ab",
   "metadata": {
    "papermill": {
     "duration": 0.122247,
     "end_time": "2022-07-21T20:13:30.567674",
     "exception": false,
     "start_time": "2022-07-21T20:13:30.445427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Rolling back the 1st difference transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965b703",
   "metadata": {
    "papermill": {
     "duration": 0.146834,
     "end_time": "2022-07-21T20:13:30.838113",
     "exception": false,
     "start_time": "2022-07-21T20:13:30.691279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rolling_back_transformation(df_train, forecast_output):\n",
    "    \"\"\"\n",
    "    Reconstructs the original forecated values by reversing first-order differencing.\n",
    "    This is done by cumulatively summing the differenced forecasts and adding the last observed value\n",
    "    from the training data for each series.\n",
    "\n",
    "    Parameters:\n",
    "        df_train (pd.DataFrame): Original training dataset, used to retrieve last known values.\n",
    "        forecast_output (pd.DataFrame): Forecasted differenced values, with column names ending in '_forecast'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Forecast with restored original scale, using cumulative sums.\n",
    "    \"\"\"\n",
    "    forecast_final = forecast_output.copy()\n",
    "\n",
    "    for col in df_train.columns:        \n",
    "        forecast_final[str(col)+'_forecast'] = df_train[col].iloc[-1] + forecast_output[str(col)+'_forecast'].cumsum()\n",
    "    \n",
    "    return forecast_final\n",
    "\n",
    "ny_forecast_final = rolling_back_transformation(ny_train, ny_forecast_output)\n",
    "ldn_forecast_final =rolling_back_transformation(ldn_train, ldn_forecast_output)\n",
    "\n",
    "ny_forecast_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd00c4",
   "metadata": {
    "papermill": {
     "duration": 7.931481,
     "end_time": "2022-07-21T20:13:38.892061",
     "exception": false,
     "start_time": "2022-07-21T20:13:30.960580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_forecast(place, col, df_test, forecast_final): \n",
    "    \"\"\"\n",
    "    Plots actual vs forecasted values for a specific time series feature in a given location.\n",
    "\n",
    "    Parameters:\n",
    "        place (str): Name of the location used in the plots title.\n",
    "        col (str): Name of the column/feature to plot.\n",
    "        df_test (pd.DataFrame): DataFrame containing actual observed values.\n",
    "        forecast_final (pd.DataFrame): DataFrame containing forecasted values.\n",
    "    \"\"\"\n",
    "    plt.rcParams.update(params)\n",
    "    sns.lineplot(x=df_test.index, y=col, data=df_test, label='Actual')\n",
    "\n",
    "    forecast_col = col+'_forecast'\n",
    "    sns.lineplot(x=forecast_final.index, y=forecast_col, data=forecast_final, label='Forecast')\n",
    "    \n",
    "    plt.title(col + \" in \" + place)\n",
    "    plt.tight_layout();\n",
    "    plt.show()\n",
    "    \n",
    "for feature in features:\n",
    "    plot_forecast(\"New York\", feature, ny_test, ny_forecast_final)\n",
    "    plot_forecast(\"London\", feature, ldn_test, ldn_forecast_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e45de4",
   "metadata": {
    "papermill": {
     "duration": 0.154599,
     "end_time": "2022-07-21T20:13:39.201382",
     "exception": false,
     "start_time": "2022-07-21T20:13:39.046783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676495f",
   "metadata": {
    "papermill": {
     "duration": 0.184127,
     "end_time": "2022-07-21T20:13:39.542657",
     "exception": false,
     "start_time": "2022-07-21T20:13:39.358530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forecast_accuracy(df_test, forecast_final, col):\n",
    "    \"\"\"\n",
    "    Calculates and prints forecast accuracy metrics for a given time series column.\n",
    "    Metrics computed:\n",
    "        - Mean Error (ME)\n",
    "        - Mean Absolute Error (MAE)\n",
    "        - Root Mean Squared Error (RMSE)\n",
    "\n",
    "    Parameters:\n",
    "        df_test (pd.DataFrame): DataFrame with actual observed values.\n",
    "        forecast_final (pd.DataFrame): DataFrame containing forecasted values.\n",
    "        col (str): Name of the column/feature to evaluate.\n",
    "    \"\"\"\n",
    "    col_forecast = col +'_forecast'\n",
    "    forecast = forecast_final[col_forecast].values\n",
    "    actual = df_test[col]\n",
    "    me = np.mean(forecast - actual)             # Mean Error\n",
    "    mae = np.mean(np.abs(forecast - actual))    # Mean Absolute Error\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # Root Mean Squared Error\n",
    "    \n",
    "    print(\"{:>18} {:>10} {: >20} {: >20}\".format(*[col, round(me,3), round(mae,3), round(rmse,3)]))\n",
    "\n",
    "def display_accuracy(df_test, forecast_final):\n",
    "    \"\"\"\n",
    "    Displays accuracy metrics for all forecasted features listed in the global 'features' variable.\n",
    "\n",
    "    Calls 'forecast_accuracy' for each feature and prints the results in a formatted table.\n",
    "\n",
    "    Parameters:\n",
    "        df_test (pd.DataFrame): DataFrame with actual observed values.\n",
    "        forecast_final (pd.DataFrame): DataFrame containing forecasted values.\n",
    "    \"\"\"\n",
    "    print(\"{:>18} {:>15} {: >25} {: >25}\".format(*[\"Series\", \"Mean Error\", \"Mean Absolute Error\", \"Root Mean Squared Error\"]))\n",
    "    for col in features:\n",
    "        forecast_accuracy(df_test, forecast_final, col)\n",
    "\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'New York:' + color.END)\n",
    "display_accuracy(ny_test, ny_forecast_final)\n",
    "print(\"\\n\\n\"+color.BOLD + color.PURPLE + 'London:' + color.END)\n",
    "display_accuracy(ldn_test, ldn_forecast_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959456a",
   "metadata": {},
   "source": [
    "## Logging Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCitiesModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self):\n",
    "        try:    \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error error during initialization: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, context, model_input, params = None):\n",
    "        \"\"\"\n",
    "        Computes the predicted class of Iris Flower.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error performing prediction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    @classmethod\n",
    "    def log_model(cls, model_name):\n",
    "        \"\"\"\n",
    "        Logs the model to MLflow with appropriate artifacts and schema.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Define input and output schema\n",
    "            input_schema = Schema([\n",
    "                ColSpec(\"string\",\"citie\"),\n",
    "                \n",
    "                ])\n",
    "            output_schema = Schema([\n",
    "                ColSpec(\"string\", \"class\"),\n",
    "            ])\n",
    "            \n",
    "            # Define model signature\n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "            \n",
    "            # Log the model in MLflow\n",
    "            mlflow.pyfunc.log_model(\n",
    "                model_name,\n",
    "                python_model=cls(),\n",
    "                signature=signature,\n",
    "\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging model: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3656fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Starting the experiment: {EXPERIMENT_NAME}')\n",
    "\n",
    "# Set the MLflow experiment name\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    # Print the artifact URI for reference\n",
    "    logging.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    \n",
    "    # Log the model to MLflow\n",
    "    TwoCitiesModel.log_model(model_name=MODEL_NAME)\n",
    "\n",
    "    # Register the logged model in MLflow Model Registry\n",
    "    mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/{MODEL_NAME}\", \n",
    "        name=MODEL_NAME\n",
    "    )\n",
    "\n",
    "logger.info(f'Registered the model: {MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878eacd7",
   "metadata": {},
   "source": [
    "## Fetching the Latest Model Version from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c82b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Retrieve the latest version of the \"Iris_Flower_Model\" model (not yet in a specific stage)\n",
    "model_metadata = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version  # Extract the latest model version\n",
    "\n",
    "# Fetch model information, including its signature\n",
    "model_info = mlflow.models.get_model_info(f\"models:/{MODEL_NAME}/{latest_model_version}\")\n",
    "\n",
    "# Print the latest model version and its signature\n",
    "print(f\"Latest Model Version: {latest_model_version}\")\n",
    "print(f\"Model Signature: {model_info.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef755e",
   "metadata": {},
   "source": [
    "## Loading the Model and Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e11a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{MODEL_NAME}/{latest_model_version}\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "\n",
    "print(\"Input:\", x_test)\n",
    "print(\"\\nPrediction:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805a67b-86b7-4b9d-979d-dafd43d7f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6f158",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using [**Z by HP AI Studio**](https://zdocs.datascience.hp.com/docs/aistudio/overview)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 54.676437,
   "end_time": "2022-07-21T20:13:40.519212",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-21T20:12:45.842775",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
