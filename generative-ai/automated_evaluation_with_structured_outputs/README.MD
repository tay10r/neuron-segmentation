# ğŸ“Š Automatedâ€¯Evaluationâ€¯withâ€¯Structuredâ€¯Outputs

# ğŸ“š Contents

* [ğŸ§  Overview](#overview)
* [ğŸ—‚ Project Structure](#project-structure)
* [âš™ï¸ Setup](#setup)
* [ğŸš€ Usage](#usage)
* [ğŸ“ Contact and Support](#contact-and-support)

---

# Overview

**Automatedâ€¯Evaluationâ€¯withâ€¯Structuredâ€¯Outputs** turns a local **Metaâ€‘Llamaâ€‘2** model into an MLflowâ€‘served scorer that rates any batch of texts (e.g., project abstracts) against arbitrary rubric criteria.
The pipeline:

* Generates scores locally viaâ€¯`llama.cpp` (no data leaves your machine)
* Registers the evaluator as a **pyfunc** model in MLflow
* Exposes a REST `/invocations` endpoint
* Ships two frontâ€‘ends â€” a **Streamlit** dashboard and a pure **HTML/JS** UI â€” for instant humanâ€‘friendly interaction and CSV download.

---

# Project Structure

```
â”œâ”€â”€ demo
â”‚   â”œâ”€â”€ index.html                 # Lightweight HTML/JS UI
â”‚   â””â”€â”€ streamlit-webapp/          # Streamlit UI
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ html_ui_for_automated_evaluation.png          # UI screenshot
â”‚   â””â”€â”€ streamlit_ui_for_automated_evaluation.png          # UI screenshot
â”‚   â””â”€â”€ successful html ui result for automated evaluation.pdf          # UI pdf
â”‚   â””â”€â”€ successful streamlit ui result for automated evaluation.pdf          # UI pdf
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ automated_evaluation_with_structured_outputs.ipynb   # Oneâ€‘click notebook to to setup the pipeline and create the api
â”‚   â””â”€â”€ isef_evaluation_with_llama.ipynb   # Oneâ€‘click notebook to evaluate isef projects with local Llama model
â”‚   â””â”€â”€ isef_evaluation_with_openai.ipynb   # Oneâ€‘click notebook to evaluate isef projects with openAI model
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# Setup

### 0â€¯â–ªâ€¯Minimum hardware

* **RAM:**â€¯64â€¯GB
* **VRAM:**â€¯12â€¯GB
* **GPU:**â€¯NVIDIA (CUDA)

### 1â€¯â–ªâ€¯Create an AIâ€¯Studio project

Log into [Zâ€¯byâ€¯HPâ€¯AIâ€¯Studio](https://zdocs.datascience.hp.com/docs/aistudio/overview) and start a **Local GenAI** workspace.

### 2â€¯â–ªâ€¯Clone the repo

```bash
git clone https://github.com/HPInc/aistudio-samples.git
```

### 3â€¯â–ªâ€¯Add the Llamaâ€‘3 model

- Download the 7-Bâ€‘Instructâ€¯GGUF via Modelsâ€¯tab:

  * **Model Name**: `llama2-7b`
  * **Model Source**: `AWS S3`
  * **S3 URI**: `s3://149536453923-hpaistudio-public-assets/llama2-7b`
  * **Bucket Region**: `us-west-2`

- Make sure that the model is in the `datafabric` folder inside your workspace. If the model does not appear after downloading, please restart your workspace.

### 4â€¯â–ªâ€¯Configure secrets (optional)

Edit `configs/secrets.yaml` if you plan to call external APIs; not required for pure local inference.

### 5â€¯â–ªâ€¯Input File

You need to setup your input files in the 5th cell to work with your input csv files and configuration.

---

# Usage

### 1â€¯â–ªâ€¯Run the notebook

Open **`notebooks/automated_evaluation_with_structured_outputs.ipynb`**, run all cells.
This will:

1. Load the GGUF model
2. Setup the pipeline
3. Log **`LlamaEvaluatorModel`** to MLflow

### 2â€¯â–ª Deploy the LlamaEvaluator Service

- Go to **Deployments > New Service** in AI Studio.
- Name the service and select the registered model.
- Choose a model version and enable **GPU acceleration**.
- Start the deployment.
- Note: This is a local deployment running on your machine. As a result, if API processing takes more than a few minutes, it may return a timeout error. If you need to work with inputs that require longer processing times, we recommend using the provided notebook in the project files instead of accessing the API via Swagger or the web app UI.

### 3â€¯â–ªâ€¯Swagger / raw API

Once deployed, access the **Swagger UI** via the Service URL.


Paste a payload like:

```jsonc
{
  "dataframe_split": {
    "columns": ["BoothNumber", "AbstractText"],
    "data": [
      ["TEST001","Microplastics impact on marine life"],
      ["TEST002","Lowâ€‘cost solar charger for offâ€‘grid use"]
    ]
  },
  "params": {
    "key_column":"BoothNumber",
    "eval_column":"AbstractText",
    "criteria":"[\"Originality\",\"ScientificRigor\",\"Clarity\",\"Relevance\",\"Feasibility\",\"Brevity\"]",
    "batch_size":2
  }
}
```

### 4â€¯â–ªâ€¯Use the HTML UI

 From the Swagger page, click the demo link to interact with the locally deployed vanilla RAG chatbot via UI.

### 5â€¯â–ªâ€¯Launch the Streamlit UI

1. To launch the Streamlit UI, follow the instructions in the README file located in the `demo/streamlit-webapp` folder.

2. Navigate to the shown URL, upload a CSV, tweak parameters, and view scores.


### Successful UI demo

- HTML
![Automated Evaluation HTML UI](docs/html_ui_for_automated_evaluation.png)  

- Streamlit
![Automated Evaluation Streamlit UI](docs/streamlit_ui_for_automated_evaluation.png)  



---

# Contact and Support

* ğŸ’¬Â Open an [issue](https://github.com/HPInc/aistudio-samples/issues) for bugs or questions.
* ğŸ“˜Â See [AIâ€¯Studio Docs](https://zdocs.datascience.hp.com/docs/aistudio/overview) for help.

---

> Built with â¤ï¸ using [**ZÂ byÂ HPÂ AIâ€¯Studio**](https://zdocs.datascience.hp.com/docs/aistudio/overview).
