# ğŸ“Š Automatedâ€¯Evaluationâ€¯withâ€¯Structuredâ€¯Outputs

# ğŸ“š Contents

* [ğŸ§  Overview](#overview)
* [ğŸ—‚ Project Structure](#project-structure)
* [âš™ï¸ Setup](#setup)
* [ğŸš€ Usage](#usage)
* [ğŸ“ Contact and Support](#contact-and-support)

---

# Overview

**Automatedâ€¯Evaluationâ€¯withâ€¯Structuredâ€¯Outputs** turns a local **Metaâ€‘Llamaâ€‘3** model into an MLflowâ€‘served scorer that rates any batch of texts (e.g., ISEF abstracts) against arbitrary rubric criteria.
The pipeline:

* Generates scores locally viaâ€¯`llama.cpp` (no data leaves your machine)
* Registers the evaluator as a **pyfunc** model in MLflow
* Exposes a REST `/invocations` endpoint
* Ships two frontâ€‘ends â€” a **Streamlit** dashboard and a pure **HTML/JS** UI â€” for instant humanâ€‘friendly interaction and CSV download.

---

# Project Structure

```
â”œâ”€â”€ core
â”‚   â””â”€â”€ llama_evaluator
â”‚       â”œâ”€â”€ model.py               # LlamaEvaluatorModel class (MLflow)
â”‚       â””â”€â”€ streamlit_app.py       # Streamlit UI
â”œâ”€â”€ demo
â”‚   â”œâ”€â”€ index.html                 # Lightweight HTML/JS UI
â”‚   â””â”€â”€ assets/
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ build_and_register.ipynb   # Oneâ€‘click notebook to train & log model
â”œâ”€â”€ datafabric/Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘â€¦   # Quantized GGUF model
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ui_llamascore.png          # UI screenshot
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# Setup

### 0â€¯â–ªâ€¯Minimum hardware

* **RAM:**â€¯32â€¯GB
* **VRAM:**â€¯10â€¯GBâ€¯(16â€¯GB+ for fastest latency)
* **GPU:**â€¯NVIDIA (CUDA)

### 1â€¯â–ªâ€¯Create an AIâ€¯Studio project

Log into [Zâ€¯byâ€¯HPâ€¯AIâ€¯Studio](https://zdocs.datascience.hp.com/docs/aistudio/overview) and start a **Local GenAI** workspace.

### 2â€¯â–ªâ€¯Clone the repo

```bash
git clone https://github.com/<yourâ€‘org>/automatedâ€‘evaluationâ€‘structuredâ€‘outs.git
```

### 3â€¯â–ªâ€¯Add the Llamaâ€‘3 model

Download the 8â€‘Bâ€‘Instructâ€¯GGUF via Modelsâ€¯tab:

* **Model name:** `Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instructâ€‘Q8_0`
* **Source:** AWSÂ S3 â†’ `s3://â€¦/Meta-Llama-3-8B-Instruct-Q8_0.gguf`

Place it under `datafabric/`.

### 4â€¯â–ªâ€¯Configure secrets (optional)

Edit `configs/secrets.yaml` if you plan to call external APIs; not required for pure local inference.

---

# Usage

### 1â€¯â–ªâ€¯Run the notebook

Open **`notebooks/build_and_register.ipynb`**, run all cells.
This will:

1. Load the GGUF model
2. Log **`LlamaEvaluatorModel`** to MLflow
3. Serve it locally on portÂ **5000**

### 2â€¯â–ªâ€¯Launch the Streamlit UI

```bash
python core/llama_evaluator/streamlit_app.py
# or
streamlit run core/llama_evaluator/streamlit_app.py
```

Navigate to the shown URL, upload a CSV, tweak parameters, and view scores.

### 3â€¯â–ªâ€¯Use the HTML UI

Serve `demo/index.html` from the same origin as your MLflow service (e.g. drop it in the modelâ€‘serve folder).
The page autoâ€‘detects the host and calls **`/invocations`**.

### 4â€¯â–ªâ€¯Swagger / raw API

If you enabled **MLflow deployments**, open Swagger at

```
http://<host>:5000/docs#/default/invocations_invocations_post
```

Paste a payload like:

```jsonc
{
  "dataframe_split": {
    "columns": ["BoothNumber", "AbstractText"],
    "data": [
      ["TEST001","Microplastics impact on marine life"],
      ["TEST002","Lowâ€‘cost solar charger for offâ€‘grid use"]
    ]
  },
  "params": {
    "key_column":"BoothNumber",
    "eval_column":"AbstractText",
    "criteria":"[\"Originality\",\"ScientificRigor\",\"Clarity\",\"Relevance\",\"Feasibility\",\"Brevity\"]",
    "batch_size":2
  }
}
```

### Successful UI demo

![LlamaScore UI](docs/ui_llamascore.png)

---

# Contact and Support

* ğŸ’¬Â Open an [issue](https://github.com/<yourâ€‘org>/automatedâ€‘evaluationâ€‘structuredâ€‘outs/issues) for bugs or questions.
* ğŸ“˜Â See [AIâ€¯Studio Docs](https://zdocs.datascience.hp.com/docs/aistudio/overview) for workspace help.

---

> Built with â¤ï¸ using [**ZÂ byÂ HPÂ AIâ€¯Studio**](https://zdocs.datascience.hp.com/docs/aistudio/overview).
