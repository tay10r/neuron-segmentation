{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a291384-cd71-4a8d-b77f-c8be54073bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0041982-b220-41fb-b9a2-11bbf3334d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Setup\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import httpx\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from llama_cpp import Llama, LlamaGrammar\n",
    "import multiprocessing\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from llama_cpp import Llama, LlamaGrammar\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec, DataType, ParamSpec, ParamSchema\n",
    "import json\n",
    "import logging\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4af9a5-ef38-4497-a29c-931329c31892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure total runtime\n",
    "start_notebook = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89e3f1b-e6ad-4e48-9130-757b282e4543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 s, sys: 6.54 s, total: 8.66 s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3. Load & Configure Local LLaMA\n",
    "#local_model_path = \"/home/jovyan/datafabric/Meta-Llama-3-8B-Instruct-Q8_0/Meta-Llama-3-8B-Instruct-Q8_0.gguf\"\n",
    "local_model_path = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "            model_path=local_model_path,\n",
    "            n_gpu_layers=-1,\n",
    "            n_batch=128,\n",
    "            n_ctx=8192,\n",
    "            max_tokens=512,\n",
    "            f16_kv=True,\n",
    "            use_mmap=True,\n",
    "            low_vram=True,\n",
    "            rope_scaling=None,\n",
    "            temperature=0.0,\n",
    "            repeat_penalty=1.0,\n",
    "            streaming=False,\n",
    "            stop=None,\n",
    "            seed=42,\n",
    "            num_threads=multiprocessing.cpu_count(),\n",
    "            verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1391c180-f33a-406e-9e4b-bda20c04405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Constants & Configuration\n",
    "INPUT_PATH    = \"../2025 ISEF Project Abstracts.csv\"  # <-- set your input CSV here\n",
    "OUTPUT_PATH   = \"../Sorted_by_Score.csv\"             # <-- set your output CSV here\n",
    "KEY_COLUMN    = \"BoothNumber\"                     # <-- unique ID column\n",
    "EVAL_COLUMN   = \"AbstractText\"                    # <-- text column to evaluate\n",
    "CRITERIA      = [                                  # <-- list your evaluation criteria\n",
    "    \"Originality\",\n",
    "    \"ScientificRigor\",\n",
    "    \"Clarity\",\n",
    "    \"Relevance\",\n",
    "    \"Feasibility\",\n",
    "    \"Brevity\",\n",
    "]\n",
    "BATCH_SIZE    = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea62050a-4ecf-4b54-81fe-6d1c74947157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have:\n",
    "#   KEY_COLUMN = \"BoothNumber\"\n",
    "#   CRITERIA   = [\"Originality\",\"ScientificRigor\",\"Clarity\",\"Relevance\",\"Feasibility\"]\n",
    "\n",
    "# Build the bullet list dynamically:\n",
    "criteria_bullets = \"\\n\".join(f\"- {c}\" for c in CRITERIA)\n",
    "\n",
    "# Build the example-object fields (all with dummy value 7):\n",
    "example_fields = \", \".join(f'\"{c}\": 7' for c in CRITERIA)\n",
    "\n",
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"You are an expert evaluator.  \"\n",
    "    f\"For each input record, score 1–10 on these criteria:\\n\"\n",
    "    f\"{criteria_bullets}\\n\\n\"\n",
    "    \"Respond *only* with a valid JSON object of the form:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"results\": [\\n'\n",
    "    f'    {{ \"{KEY_COLUMN}\": \"...\", {example_fields} }}\\n'\n",
    "    \"  ]\\n\"\n",
    "    \"}\\n\"\n",
    "    \"Do not include any other text, explanation, or markup.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37962aac-8800-4b53-b890-1f9bdbfa6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Helper Functions\n",
    "\n",
    "def chunk_list(lst: List[int], size: int) -> List[List[int]]:\n",
    "    return [lst[i : i + size] for i in range(0, len(lst), size)]\n",
    "\n",
    "# Load the “json_arr” grammar for a top‐level JSON array\n",
    "GRAMMAR_URL = \"https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json_arr.gbnf\"\n",
    "grammar_text = httpx.get(GRAMMAR_URL).text\n",
    "json_arr_grammar = LlamaGrammar.from_string(grammar_text)\n",
    "\n",
    "def evaluate_batch(batch_df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scores a batch of rows under the json_arr grammar,\n",
    "    returning a flat list of dicts with KEY_COLUMN + CRITERIA keys.\n",
    "    \"\"\"\n",
    "    payload = [\n",
    "        {KEY_COLUMN: str(r[KEY_COLUMN]), EVAL_COLUMN: r[EVAL_COLUMN]}\n",
    "        for _, r in batch_df.iterrows()\n",
    "    ]\n",
    "    prompt = SYSTEM_INSTRUCTIONS + \"\\n\\n\" + json.dumps(payload, indent=2)\n",
    "\n",
    "    resp: Dict[str, Any] = llm(\n",
    "        prompt,\n",
    "        grammar=json_arr_grammar,\n",
    "        #grammar=objarr_grammar,   # ← now only allows [ { … }, { … } ]\n",
    "        max_tokens=-1,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    # Extract text\n",
    "    text = resp[\"choices\"][0][\"text\"]\n",
    "    data = json.loads(text)\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    else:\n",
    "        raise RuntimeError(f\"Expected JSON array, got {type(data)}:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "776b6999-a68e-440b-8c53-4db63a17cf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef843efe124403797e30206e73ac0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring batches:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Load Data & Validate Columns\n",
    "df = pd.read_csv(INPUT_PATH)[:10]\n",
    "for col in (KEY_COLUMN, EVAL_COLUMN):\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Required column '{col}' not found in input CSV\")\n",
    "\n",
    "df[KEY_COLUMN] = df[KEY_COLUMN].astype(str)\n",
    "\n",
    "# 6. Batch Evaluation Loop\n",
    "results: List[Dict[str, Any]] = []\n",
    "for batch_idxs in tqdm(\n",
    "    chunk_list(df.index.tolist(), BATCH_SIZE),\n",
    "    desc=\"Scoring batches\",\n",
    "    unit=\"batch\"\n",
    "):\n",
    "    batch_df      = df.loc[batch_idxs]\n",
    "    batch_results = evaluate_batch(batch_df)\n",
    "    results.extend(batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c289ad70-280a-418f-996a-9096d0fe9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done in 265.3s — output saved to '../Sorted_by_Score.csv'\n"
     ]
    }
   ],
   "source": [
    "# 7. Flatten nested batch results into a flat list of record‐dicts\n",
    "flat_results: List[Dict[str, Any]] = []\n",
    "for batch in results:\n",
    "    # If each batch is a dict with a \"results\" key, use that list\n",
    "    if isinstance(batch, dict) and \"results\" in batch and isinstance(batch[\"results\"], list):\n",
    "        flat_results.extend(batch[\"results\"])\n",
    "    # If somehow you ended up with lists directly, handle those too\n",
    "    elif isinstance(batch, list):\n",
    "        flat_results.extend(batch)\n",
    "    else:\n",
    "        # Ignore anything else (e.g. stray floats)\n",
    "        logging.warning(f\"Ignoring unexpected batch entry: {batch!r}\")\n",
    "\n",
    "# 8. Build the scores DataFrame\n",
    "scores_df = pd.DataFrame(flat_results)\n",
    "\n",
    "# 9. Sanity check: ensure your key column is present\n",
    "if KEY_COLUMN not in scores_df.columns:\n",
    "    raise KeyError(\n",
    "        f\"Expected column '{KEY_COLUMN}' in scores_df, but got: {scores_df.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "# 10. Cast keys to string on both sides\n",
    "scores_df[KEY_COLUMN] = scores_df[KEY_COLUMN].astype(str)\n",
    "df[KEY_COLUMN]      = df[KEY_COLUMN].astype(str)\n",
    "\n",
    "# 11. Merge, compute TotalScore, sort, and export\n",
    "combined = df.merge(scores_df, on=KEY_COLUMN, how=\"left\")\n",
    "combined[\"TotalScore\"] = combined[CRITERIA].sum(axis=1)\n",
    "combined.sort_values(\"TotalScore\", ascending=False, inplace=True)\n",
    "combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "combined.to_csv(OUTPUT_PATH, index=False)\n",
    "elapsed = time.time() - start_notebook\n",
    "print(f\"✅ Done in {elapsed:.1f}s — output saved to '{OUTPUT_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0423bf9-f0ba-42f0-843f-10acee4c44dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoothNumber</th>\n",
       "      <th>ParentCategory</th>\n",
       "      <th>ProjectTitle</th>\n",
       "      <th>AbstractText</th>\n",
       "      <th>Originality</th>\n",
       "      <th>ScientificRigor</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Feasibility</th>\n",
       "      <th>Brevity</th>\n",
       "      <th>TotalScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANIM001</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Investigating the Synergistic Effects of High-...</td>\n",
       "      <td>The project targeted two specific nutrients ty...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANIM005</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>PawPath: An IMU-Based Gait Detection and Disea...</td>\n",
       "      <td>PawPath is a non-invasive, risk-free gait moni...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANIM002</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Evaluating the Efficacy of Novel Carbon Dioxid...</td>\n",
       "      <td>Honeybees are indispensable pollinators, contr...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANIM004T</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Tube-Worm Hunters: Ecological Aspects of Ficop...</td>\n",
       "      <td>Non-native species pose a global threat to aqu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANIM003</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Circadian Evolution in Action: How Latitude Sh...</td>\n",
       "      <td>The circadian rhythm is a 24-hour biological c...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANIM006</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Tailsense: Classifying Dogs' Emotions Using Ba...</td>\n",
       "      <td>Purpose\\r\\nDog barks and visual cues are main ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANIM007</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Investigating the Infection Rates of Hector's ...</td>\n",
       "      <td>Hector’s lantern fish (Lampanyctodes hectoris)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANIM008T</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Full Observation System for Monitoring Animal ...</td>\n",
       "      <td>In many farms around the world, production is ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANIM009</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Behind the Banner: Antibiotic Resistance in Li...</td>\n",
       "      <td>The purpose of this study was to investigat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANIM010</td>\n",
       "      <td>Animal Sciences</td>\n",
       "      <td>Generating RNAi Pesticides to Specifically Tar...</td>\n",
       "      <td>Fire ants cause billions of dollars of economi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BoothNumber   ParentCategory  \\\n",
       "0     ANIM001  Animal Sciences   \n",
       "1     ANIM005  Animal Sciences   \n",
       "2     ANIM002  Animal Sciences   \n",
       "3    ANIM004T  Animal Sciences   \n",
       "4     ANIM003  Animal Sciences   \n",
       "5     ANIM006  Animal Sciences   \n",
       "6     ANIM007  Animal Sciences   \n",
       "7    ANIM008T  Animal Sciences   \n",
       "8     ANIM009  Animal Sciences   \n",
       "9     ANIM010  Animal Sciences   \n",
       "\n",
       "                                        ProjectTitle  \\\n",
       "0  Investigating the Synergistic Effects of High-...   \n",
       "1  PawPath: An IMU-Based Gait Detection and Disea...   \n",
       "2  Evaluating the Efficacy of Novel Carbon Dioxid...   \n",
       "3  Tube-Worm Hunters: Ecological Aspects of Ficop...   \n",
       "4  Circadian Evolution in Action: How Latitude Sh...   \n",
       "5  Tailsense: Classifying Dogs' Emotions Using Ba...   \n",
       "6  Investigating the Infection Rates of Hector's ...   \n",
       "7  Full Observation System for Monitoring Animal ...   \n",
       "8  Behind the Banner: Antibiotic Resistance in Li...   \n",
       "9  Generating RNAi Pesticides to Specifically Tar...   \n",
       "\n",
       "                                        AbstractText  Originality  \\\n",
       "0  The project targeted two specific nutrients ty...          8.0   \n",
       "1  PawPath is a non-invasive, risk-free gait moni...          8.0   \n",
       "2  Honeybees are indispensable pollinators, contr...          7.0   \n",
       "3  Non-native species pose a global threat to aqu...          7.0   \n",
       "4  The circadian rhythm is a 24-hour biological c...          6.0   \n",
       "5  Purpose\\r\\nDog barks and visual cues are main ...          NaN   \n",
       "6  Hector’s lantern fish (Lampanyctodes hectoris)...          NaN   \n",
       "7  In many farms around the world, production is ...          NaN   \n",
       "8     The purpose of this study was to investigat...          NaN   \n",
       "9  Fire ants cause billions of dollars of economi...          NaN   \n",
       "\n",
       "   ScientificRigor  Clarity  Relevance  Feasibility  Brevity  TotalScore  \n",
       "0              8.0      8.0        8.0          8.0      8.0        48.0  \n",
       "1              8.0      8.0        8.0          8.0      8.0        48.0  \n",
       "2              7.0      7.0        7.0          7.0      7.0        42.0  \n",
       "3              7.0      7.0        7.0          7.0      7.0        42.0  \n",
       "4              6.0      6.0        6.0          6.0      6.0        36.0  \n",
       "5              NaN      NaN        NaN          NaN      NaN         0.0  \n",
       "6              NaN      NaN        NaN          NaN      NaN         0.0  \n",
       "7              NaN      NaN        NaN          NaN      NaN         0.0  \n",
       "8              NaN      NaN        NaN          NaN      NaN         0.0  \n",
       "9              NaN      NaN        NaN          NaN      NaN         0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd745cae-26ae-42e1-8658-4a77a35cd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR_URL = (\n",
    "    \"https://raw.githubusercontent.com/ggerganov/llama.cpp/\"\n",
    "    \"master/grammars/json_arr.gbnf\"\n",
    ")\n",
    "\n",
    "class LlamaEvaluatorModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    A PythonModel that uses a local LLaMA model to score texts on multiple criteria.\n",
    "\n",
    "    Predict signature:\n",
    "      predict(self, context, model_input: DataFrame, params: Dict[str,Any])\n",
    "    where `params` must include:\n",
    "      - key_column (str)\n",
    "      - eval_column (str)\n",
    "      - criteria (JSON-encoded list of str)\n",
    "      - batch_size (int)\n",
    "    \"\"\"\n",
    "    def load_context(self, context):\n",
    "        # 1. Load LLaMA model\n",
    "        model_path = context.artifacts[\"llama_model_path\"]\n",
    "        self.llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=-1,\n",
    "            n_batch=128,\n",
    "            n_ctx=8192,\n",
    "            max_tokens=512,\n",
    "            f16_kv=True,\n",
    "            use_mmap=True,\n",
    "            low_vram=True,\n",
    "            rope_scaling=None,\n",
    "            temperature=0.0,\n",
    "            repeat_penalty=1.0,\n",
    "            streaming=False,\n",
    "            stop=None,\n",
    "            seed=42,\n",
    "            num_threads=multiprocessing.cpu_count(),\n",
    "            verbose=False,\n",
    "        )\n",
    "        # 2. Load JSON-array grammar\n",
    "        grammar_text = httpx.get(GRAMMAR_URL).text\n",
    "        self.grammar = LlamaGrammar.from_string(grammar_text)\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame, params: Dict[str,Any]) -> pd.DataFrame:\n",
    "        # 1. Extract config from params\n",
    "        try:\n",
    "            key_column   = params[\"key_column\"]\n",
    "            eval_column  = params[\"eval_column\"]\n",
    "            criteria     = params[\"criteria\"]\n",
    "            batch_size   = int(params[\"batch_size\"])\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Missing required param: {e}\")\n",
    "\n",
    "        # If criteria passed as JSON string, parse it\n",
    "        if isinstance(criteria, str):\n",
    "            criteria = json.loads(criteria)\n",
    "        # 2. Validate DataFrame columns\n",
    "        for col in (key_column, eval_column):\n",
    "            if col not in model_input.columns:\n",
    "                raise KeyError(f\"Input DataFrame must contain column '{col}'\")\n",
    "\n",
    "        df = model_input.copy()\n",
    "        df[key_column] = df[key_column].astype(str)\n",
    "        # 3. Build prompt template\n",
    "        bullets = \"\\n\".join(f\"- {c}\" for c in criteria)\n",
    "        example_fields = \", \".join(f'\"{c}\": 7' for c in criteria)\n",
    "        prompt_template = (\n",
    "            \"You are an expert evaluator. For each input record, \"\n",
    "            \"score 1–10 on these criteria:\\n\"\n",
    "            f\"{bullets}\\n\\n\"\n",
    "            \"Respond *ONLY* with a JSON array of objects. Each element *MUST* be an object containing the *EXACT* fields shown below;\"\n",
    "            \"*NEVER* output numbers, strings, or null values.\\n\"\n",
    "            \"[\\n\"\n",
    "            f'  {{ \"{key_column}\": \"...\", {example_fields} }},\\n'\n",
    "            \"  { … }\\n\"\n",
    "            \"]\\n\"\n",
    "            \"No wrapper, no extra text.\"\n",
    "            \"Do not include any other text, explanation, or markup.\"\n",
    "            \"Return *ONLY* with a JSON array of objects.\"\n",
    "        )\n",
    "\n",
    "        prompt_template = (\n",
    "            \"You are an expert evaluator.  \"\n",
    "            f\"For each input record, score 1–10 on these criteria:\\n\"\n",
    "            f\"{bullets}\\n\\n\"\n",
    "            \"Respond *only* with a valid JSON object of the form:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"results\": [\\n'\n",
    "            f'    {{ \"{key_column}\": \"...\", {example_fields} }}\\n'\n",
    "            \"  ]\\n\"\n",
    "            \"}\\n\"\n",
    "            \"Do not include any other text, explanation, or markup.\"\n",
    "        )\n",
    "        # 4. Helper to chunk indices\n",
    "        def chunk_list(lst: List[int], n: int):\n",
    "            for i in range(0, len(lst), n):\n",
    "                yield lst[i : i + n]\n",
    "        # 5. Score in batches\n",
    "        scored: List[Dict[str,Any]] = []\n",
    "        for idxs in chunk_list(df.index.tolist(), batch_size):\n",
    "            batch = df.loc[idxs]\n",
    "            payload = [\n",
    "                {key_column: r[key_column], eval_column: r[eval_column]}\n",
    "                for _, r in batch.iterrows()\n",
    "            ]\n",
    "            prompt = prompt_template + \"\\n\\n\" + json.dumps(payload, indent=2)\n",
    "            resp = self.llm(prompt, grammar=self.grammar, max_tokens=-1, temperature=0.0)\n",
    "            arr = json.loads(resp[\"choices\"][0][\"text\"])\n",
    "            if not isinstance(arr, list):\n",
    "                raise RuntimeError(f\"Expected JSON array, got {type(arr)}:\\n{arr!r}\")\n",
    "            scored.extend(arr)\n",
    "        # 6. Flatten & clean model output\n",
    "        flat: List[Dict[str, Any]] = []\n",
    "        for item in scored:\n",
    "            if isinstance(item, dict) and \"results\" in item:\n",
    "                flat.extend(item[\"results\"])\n",
    "            elif isinstance(item, dict):\n",
    "                flat.append(item)\n",
    "            elif isinstance(item, list):\n",
    "                # e.g. model returned nested array: flatten one level\n",
    "                flat.extend(obj for obj in item if isinstance(obj, dict))\n",
    "            else:\n",
    "                # Skip numbers, nulls, etc.\n",
    "                logging.warning(\"Discarding non‑object item from model output: %r\", item)\n",
    "        \n",
    "        if not flat:\n",
    "            raise RuntimeError(\"Model returned no valid score objects; check prompt/grammar.\")\n",
    "        # 7. Build scores DataFrame\n",
    "        scores_df = pd.DataFrame(flat)\n",
    "        if key_column not in scores_df.columns:\n",
    "            raise KeyError(f\"Missing '{key_column}' in scored output\")\n",
    "\n",
    "        scores_df[key_column] = scores_df[key_column].astype(str)\n",
    "        # 8. Merge & compute TotalScore\n",
    "        combined = df.merge(scores_df, on=key_column, how=\"left\")\n",
    "        combined[\"TotalScore\"] = combined[criteria].sum(axis=1)\n",
    "        return combined\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(\n",
    "        cls,\n",
    "        model_name: str,\n",
    "        llama_model_path: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the model to MLflow with signature requiring:\n",
    "          - DataFrame input with any columns\n",
    "          - params: key_column (str), eval_column (str), criteria (JSON string), batch_size (int)\n",
    "        \"\"\"\n",
    "        DEMO_PATH = \"../demo\"\n",
    "        artifacts = {\n",
    "            \"llama_model_path\": llama_model_path,\n",
    "            \"demo\": DEMO_PATH,\n",
    "                    }\n",
    "\n",
    "        # Input schema: DataFrame only\n",
    "        input_schema = None  # allow arbitrary columns\n",
    "\n",
    "        # Output schema: will match input DF plus criteria columns + TotalScore\n",
    "        # we omit explicit output schema for flexibility\n",
    "\n",
    "        # Params schema: four required params\n",
    "        params_schema = ParamSchema([\n",
    "            ParamSpec(\"key_column\",  DataType.string,  None),\n",
    "            ParamSpec(\"eval_column\", DataType.string,  None),\n",
    "            ParamSpec(\"criteria\",    DataType.string,  '[\"Originality\",\"Clarity\",\"Relevance\",\"Feasibility\",\"Feasibility\"]'),\n",
    "            ParamSpec(\"batch_size\",  DataType.long,    5),\n",
    "        ])\n",
    "\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=None, params=params_schema)\n",
    "\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            registered_model_name=model_name,\n",
    "        )\n",
    "        logging.info(f\"Logged LlamaEvaluatorModel '{model_name}' requiring key_column, eval_column, criteria, batch_size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c200dfb2-97a6-4907-94e9-39e3469fd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 ──────────────────────────────────────────────────────────────────────────\n",
    "# global settings\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"llm‑mlflow\")\n",
    "\n",
    "EXPERIMENT_NAME = \"LLaMA_Evaluator_Experiment\"\n",
    "RUN_NAME        = \"LLaMA_Evaluator_Run\"\n",
    "MODEL_NAME      = \"LlamaEvaluatorModel\"\n",
    "#LLAMA_GGUF_PATH = \"/home/jovyan/datafabric/Meta-Llama-3.1-8B-Instruct-Q8_0/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\n",
    "LLAMA_GGUF_PATH = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5715c4b-0c00-4ca1-b199-b49fb9b4398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm‑mlflow:Starting experiment: LLaMA_Evaluator_Experiment\n",
      "INFO:llm‑mlflow:Run ID: cd2332b4eb1b4459983c64c6b7bea1aa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70aedd84fdf84687a887ce909a83258e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da82cbe2cbc40e69ca6e1c4ecf07d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LlamaEvaluatorModel' already exists. Creating a new version of this model...\n",
      "Created version '31' of model 'LlamaEvaluatorModel'.\n",
      "INFO:root:Logged LlamaEvaluatorModel 'LlamaEvaluatorModel' requiring key_column, eval_column, criteria, batch_size\n",
      "Registered model 'LlamaEvaluatorModel' already exists. Creating a new version of this model...\n",
      "Created version '32' of model 'LlamaEvaluatorModel'.\n",
      "INFO:llm‑mlflow:Registered model: LlamaEvaluatorModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 811 ms, sys: 9.4 s, total: 10.2 s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 2 ──────────────────────────────────────────────────────────────────────────\n",
    "# Log and register the model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "logger.info(f\"Starting experiment: {EXPERIMENT_NAME}\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    run_id = run.info.run_id\n",
    "    logger.info(\"Run ID: %s\", run_id)\n",
    "\n",
    "    # log_model now only needs model_name & gguf path (all runtime settings come as params at inference)\n",
    "    LlamaEvaluatorModel.log_model(\n",
    "        model_name      = MODEL_NAME,\n",
    "        llama_model_path= LLAMA_GGUF_PATH,\n",
    "    )\n",
    "\n",
    "    # Ensure the model is registered (in case autolog didn't)\n",
    "    mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run_id}/{MODEL_NAME}\",\n",
    "        name     =MODEL_NAME\n",
    "    )\n",
    "    logger.info(\"Registered model: %s\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2cd9b62-51b8-4ca0-8257-e85fd012bf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_942/2753195570.py:5: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest_version = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0].version\n",
      "INFO:llm‑mlflow:Latest model version: 32\n",
      "INFO:llm‑mlflow:Model signature:\n",
      "inputs: \n",
      "  None\n",
      "outputs: \n",
      "  None\n",
      "params: \n",
      "  ['key_column': string (default: None), 'eval_column': string (default: None), 'criteria': string (default: [\"Originality\",\"Clarity\",\"Relevance\",\"Feasibility\",\"Feasibility\"]), 'batch_size': long (default: 5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 ──────────────────────────────────────────────────────────────────────────\n",
    "# Retrieve the latest version & signature\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "client = MlflowClient()\n",
    "latest_version = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0].version\n",
    "logger.info(\"Latest model version: %s\", latest_version)\n",
    "\n",
    "mi = mlflow.models.get_model_info(f\"models:/{MODEL_NAME}/{latest_version}\")\n",
    "logger.info(\"Model signature:\\n%s\", mi.signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da553bf5-b034-41bb-b84b-9b3968808ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json_arr.gbnf \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.45 s, sys: 6.08 s, total: 12.5 s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4 ──────────────────────────────────────────────────────────────────────────\n",
    "# Load the model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "model_uri = f\"models:/{MODEL_NAME}/{latest_version}\"\n",
    "model     = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec726daf-a05c-42b0-b7d7-1c4d13018f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm‑mlflow:Inference results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 0 ns, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoothNumber</th>\n",
       "      <th>AbstractText</th>\n",
       "      <th>Originality</th>\n",
       "      <th>ScientificRigor</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Feasibility</th>\n",
       "      <th>Brevity</th>\n",
       "      <th>TotalScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST001</td>\n",
       "      <td>Investigating the effects of microplastics on ...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST002</td>\n",
       "      <td>Developing a low‑cost solar charger for off‑gr...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BoothNumber                                       AbstractText  Originality  \\\n",
       "0     TEST001  Investigating the effects of microplastics on ...            7   \n",
       "1     TEST002  Developing a low‑cost solar charger for off‑gr...            8   \n",
       "\n",
       "   ScientificRigor  Clarity  Relevance  Feasibility  Brevity  TotalScore  \n",
       "0                7        7          7            7        7          42  \n",
       "1                8        8          8            8        8          48  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 5 ──────────────────────────────────────────────────────────────────────────\n",
    "# Run inference\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "sample_df = pd.DataFrame({\n",
    "    \"BoothNumber\": [\"TEST001\", \"TEST002\"],\n",
    "    \"AbstractText\": [\n",
    "        \"Investigating the effects of microplastics on marine life populations.\",\n",
    "        \"Developing a low‑cost solar charger for off‑grid applications.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "predictions = model.predict(\n",
    "    sample_df,\n",
    "    params={\n",
    "        \"key_column\":  \"BoothNumber\",\n",
    "        \"eval_column\": \"AbstractText\",\n",
    "        \"criteria\":    json.dumps(\n",
    "            [\"Originality\", \"ScientificRigor\", \"Clarity\",\n",
    "             \"Relevance\", \"Feasibility\", \"Brevity\"]\n",
    "        ),\n",
    "        \"batch_size\":  5\n",
    "    }\n",
    ")\n",
    "\n",
    "logger.info(\"Inference results:\")\n",
    "\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
