{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec84ac00-1a02-4fe0-af00-31c2232ba831",
   "metadata": {},
   "source": [
    "# Interactive ORPO Fine-Tuning & Inference Hub for Open LLMs\n",
    "\n",
    "This experiment provides an interactive and modular interface for selecting, downloading, fine-tuning, and evaluating large language models using ORPO (Optimal Reward Preferring Optimization).\n",
    "The user can choose between state-of-the-art open LLMs like Mistral, LLaMA 2/3, and Gemma. \n",
    "\n",
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54051b27-5b97-47cd-89b2-4f2861711f14",
   "metadata": {},
   "source": [
    "## üì¶ Imports\n",
    "\n",
    "By using our Local GenAI workspace image, most of the necessary libraries to work with ORPO-based fine-tuning and evaluation already come pre-installed. In this notebook, we only need to import components for model loading, quantization, inference, and feedback visualization to run the complete ORPO workflow locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be25e3e1-7f9c-4536-8304-c9f1762a8f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "380f85cd-8e76-4f3e-8022-c8ad210a72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define the relative path to the 'src' directory (two levels up from current working directory)\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "# Add 'src' directory to system path for module imports (e.g., utils)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b15f11-b47a-4735-ab36-1063e88489f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üß† Core Libraries\n",
    "# ===============================\n",
    "import torch\n",
    "import multiprocessing\n",
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ===============================\n",
    "# üß™ Hugging Face & Transformers\n",
    "# ===============================\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# üß© Fine-tuning (ORPO + PEFT)\n",
    "# ===============================\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "# ===============================\n",
    "# üß∞ Project Modules: Core Pipeline\n",
    "# ===============================\n",
    "from core.selection.model_selection import ModelSelector\n",
    "from core.local_inference.inference import InferenceRunner\n",
    "from core.target_mapper.lora_target_mapper import LoRATargetMapper\n",
    "from core.data_visualizer.feedback_visualizer import UltraFeedbackVisualizer\n",
    "from core.finetuning_inference.inference_runner import AcceleratedInferenceRunner\n",
    "from core.merge_model.merge_lora import merge_lora_and_save\n",
    "from core.quantization.quantization_config import QuantizationSelector\n",
    "\n",
    "# ===============================\n",
    "# üöÄ Deployment & Evaluation\n",
    "# ===============================\n",
    "from core.deploy.deploy_fine_tuning import register_llm_comparison_model\n",
    "from core.comparer.galileo_hf_model_comparer import GalileoLocalComparer\n",
    "import promptquality as pq\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è Utility Functions\n",
    "# ===============================\n",
    "from src.utils import (\n",
    "    load_config_and_secrets,\n",
    "    configure_proxy,\n",
    "    setup_galileo_environment,\n",
    "    initialize_galileo_evaluator,\n",
    "    initialize_galileo_protect,\n",
    "    initialize_galileo_observer,\n",
    "    login_huggingface\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a13c8-7f4d-4942-8edd-888695bba2a4",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a97b90b-ed6f-4546-86d5-59279ed7ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../configs/config.yaml\"\n",
    "SECRETS_PATH = \"../../configs/secrets.yaml\"\n",
    "GALILEO_EVALUATE_PROJECT_NAME=\"AIStudio-Fine-Tuning-Evaluate\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"AIStudio-Fine-Tuning-Experiment\"\n",
    "MLFLOW_RUN_NAME = \"AIStudio-Fine-Tuning-Run\"\n",
    "MLFLOW_MODEL_NAME = \"AIStudio-Fine-Tuning-Model\"\n",
    "MODEL_SERVICE_RUN_NAME=\"AIStudio-Fine-Tuning-Service-Run\"\n",
    "MODEL_SERVICE_NAME=\"AIStudio-Fine-Tuning-Model\"\n",
    "MODEL_SERVICE_EXPERIMENT_NAME=\"AIStudio-Fine-Tuning-Experiment\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50143f8-ea8e-4e16-a33d-2a4b5f397222",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "In order to connect to Galileo service, a SSH connection needs to be established. For certain enterprise networks, this might require an explicit setup of the proxy configuration. If this is your case, set up the \"proxy\" field on your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbd88721-583c-47d9-9d50-fa2f232ab176",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_proxy(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a4dd8-5f6d-4158-bbe6-6500d3e1ccd4",
   "metadata": {},
   "source": [
    "### üîç Model Selector\n",
    "\n",
    "Below are the available models for fine-tuning with ORPO.  \n",
    "> ‚ö†Ô∏è **Note:** Make sure your Hugging Face account has access permissions for the selected model (some require manual approval).\n",
    "\n",
    "| Model ID | Hugging Face Link |\n",
    "|----------|-------------------|\n",
    "| `mistralai/Mistral-7B-Instruct-v0.1` | [üîó View on Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) |\n",
    "| `meta-llama/Llama-2-7b-chat-hf` | [üîó View on Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) |\n",
    "| `meta-llama/Meta-Llama-3-8B-Instruct` | [üîó View on Hugging Face](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |\n",
    "| `google/gemma-7b-it` | [üîó View on Hugging Face](https://huggingface.co/google/gemma-7b-it) |\n",
    "| `google/gemma-3-1b-it` | [üîó View on Hugging Face](https://huggingface.co/google/gemma-3-1b-it) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14b8e20b-84b3-4837-9727-fd959e980354",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL =  \"google/gemma-3-1b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6cfd57-da28-49dd-b6b6-50ca4189c766",
   "metadata": {},
   "source": [
    "### üîê Login to Hugging Face\n",
    "\n",
    "To access gated models (e.g., LLaMA, Mistral, or Gemma), you must authenticate using your Hugging Face token.\n",
    "\n",
    "Make sure your `secrets.yaml` file contains the following key:\n",
    "\n",
    "```yaml\n",
    "HUGGINGFACE_API_KEY: your_huggingface_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c965d03b-bf88-4caf-a736-e6c52aefc277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged into Hugging Face successfully.\n"
     ]
    }
   ],
   "source": [
    "config, secrets = load_config_and_secrets()\n",
    "login_huggingface(secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825c882-1649-4f5c-94de-5b405a5af1c0",
   "metadata": {},
   "source": [
    "### Attention Optimization Config\n",
    "Automatically selects the most efficient attention implementation and data type (dtype) based on the GPU‚Äôs compute capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cace749-80b9-4e5a-8a45-c40df3521dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c92c03-006e-4316-a411-2f8a3960590c",
   "metadata": {},
   "source": [
    "## Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4caff0c-9f4a-40cb-b0d1-49a1afccbd6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ModelSelector:[ModelSelector] Selected model: google/gemma-3-1b-it\n",
      "INFO:ModelSelector:[ModelSelector] Downloading model snapshot to: ../../../local/models/google__gemma-3-1b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122b3e98a9994197bd9ba75b2c6fc5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ModelSelector:[ModelSelector] ‚úÖ Model downloaded successfully to: ../../../local/models/google__gemma-3-1b-it\n",
      "INFO:ModelSelector:[ModelSelector] Loading model and tokenizer from: ../../../local/models/google__gemma-3-1b-it\n",
      "INFO:ModelSelector:[ModelSelector] Checking model for ORPO compatibility...\n",
      "INFO:ModelSelector:[ModelSelector] ‚úÖ Model 'google/gemma-3-1b-it' is ORPO-compatible.\n"
     ]
    }
   ],
   "source": [
    "selector = ModelSelector()\n",
    "selector.select_model(MODEL)\n",
    "\n",
    "model = selector.get_model()\n",
    "tokenizer = selector.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f763541-d805-4110-bec6-cb2c94c72e14",
   "metadata": {},
   "source": [
    "## ü§ñ Inference with Default Model\n",
    "\n",
    "The following cell runs inference using the base (non fine-tuned) model you selected earlier.\n",
    "\n",
    "We've prepared a few prompts to test different types of reasoning and writing skills.  \n",
    "You can later compare these outputs with the results generated by the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eee80d8-b12d-46a7-b057-718cf8dc5079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:InferenceRunner:[InferenceRunner] Detected 2 GPUs, loading multi-GPU configuration.\n",
      "INFO:InferenceRunner:[InferenceRunner] Loading model and tokenizer from snapshot at: ../../../local/models/google__gemma-3-1b-it\n",
      "INFO:InferenceRunner:[InferenceRunner] Running inference on input: I need to write some nodejs code that publishes a message to a Telegram group....\n",
      "INFO:InferenceRunner:[InferenceRunner] Inference completed.\n",
      "INFO:InferenceRunner:[InferenceRunner] Running inference on input: What advice would you give to a frontend developer?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Prompt 1: I need to write some nodejs code that publishes a message to a Telegram group.\n",
      "üîΩ Model Response:\n",
      "I need to write some nodejs code that publishes a message to a Telegram group.\n",
      "\n",
      "Here's the code:\n",
      "\n",
      "```javascript\n",
      "const Telegram = require('node-telegram-bot-api');\n",
      "\n",
      "// Replace with your Telegram bot token\n",
      "const token = 'YOUR_BOT_TOKEN';\n",
      "\n",
      "// Create a new bot instance\n",
      "const bot = new Telegram.Bot(token);\n",
      "\n",
      "// Replace with your Telegram group ID\n",
      "const groupId = 'YOUR_GROUP_ID';\n",
      "\n",
      "// Send the message\n",
      "bot.sendMessage(groupId, 'Hello, this is a\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:InferenceRunner:[InferenceRunner] Inference completed.\n",
      "INFO:InferenceRunner:[InferenceRunner] Running inference on input: Propose a solution that could reduce the rate of deforestation....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Prompt 2: What advice would you give to a frontend developer?\n",
      "üîΩ Model Response:\n",
      "What advice would you give to a frontend developer?\n",
      "\n",
      "Okay, here's a breakdown of advice I'd give a frontend developer, categorized for clarity:\n",
      "\n",
      "**1. Foundational Skills - The Bedrock:**\n",
      "\n",
      "* **Master the Basics:**  HTML, CSS, and JavaScript are *everything*. Don't skip them. Understand the fundamentals: DOM manipulation, event handling, basic CSS selectors, and how JavaScript works.\n",
      "* **Version Control (Git):**  This is non-negotiable. Learn to use Git and GitHub\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:InferenceRunner:[InferenceRunner] Inference completed.\n",
      "INFO:InferenceRunner:[InferenceRunner] Running inference on input: Write a eulogy for a public figure who inspired you....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Prompt 3: Propose a solution that could reduce the rate of deforestation.\n",
      "üîΩ Model Response:\n",
      "Propose a solution that could reduce the rate of deforestation.\n",
      "\n",
      "**Proposed Solution: Integrated Agroforestry with Community-Based Monitoring and Incentives**\n",
      "\n",
      "This solution combines several approaches to tackle deforestation, focusing on sustainable land management, community engagement, and economic benefits.\n",
      "\n",
      "**1. Integrated Agroforestry (IA):**\n",
      "\n",
      "* **What it is:** IA involves integrating trees and shrubs into agricultural systems. This is not just planting trees; it‚Äôs creating a productive ecosystem that provides multiple benefits ‚Äì food, timber, fuelwood, soil stabilization, and carbon sequestration.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:InferenceRunner:[InferenceRunner] Inference completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Prompt 4: Write a eulogy for a public figure who inspired you.\n",
      "üîΩ Model Response:\n",
      "Write a eulogy for a public figure who inspired you.\n",
      "\n",
      "---\n",
      "\n",
      "The air feels‚Ä¶ quiet. A quiet that isn‚Äôt peaceful, but a quiet born of absence. We‚Äôre here today to say goodbye to [Name], a voice that resonated with so much passion, so much hope, and so much unwavering belief in the power of human connection.\n",
      "\n",
      "[Name] wasn‚Äôt a politician, a celebrity, or a titan of industry. They were, simply, [Describe their core identity - e.g., a teacher, a community\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# üìã Custom prompts for evaluation\n",
    "prompts = [\n",
    "    \"I need to write some nodejs code that publishes a message to a Telegram group.\",\n",
    "    \"What advice would you give to a frontend developer?\",\n",
    "    \"Propose a solution that could reduce the rate of deforestation.\",\n",
    "    \"Write a eulogy for a public figure who inspired you.\"\n",
    "]\n",
    "\n",
    "# ‚öôÔ∏è Run inference with the selected model\n",
    "runner = InferenceRunner(selector)\n",
    "\n",
    "for idx, prompt in enumerate(prompts, 1):\n",
    "    response = runner.infer(prompt)\n",
    "    print(f\"\\nüü¢ Prompt {idx}: {prompt}\\nüîΩ Model Response:\\n{response}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185a001-2ed0-481f-8b3c-b7651ff8523d",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Creating the Fine-Tuned Model Name (ORPO)\n",
    "\n",
    "We define a clean and consistent name for the fine-tuned version of the selected base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a08c6b04-0354-4fe6-aa6e-98fd08ca08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = selector.model_id\n",
    "model_path = selector.format_model_path(base_model)\n",
    "new_model = f\"Orpo-{base_model.split('/')[-1]}-FT\"\n",
    "fine_tuned_name = f\"Orpo-{base_model.split('/')[-1]}-FT\"\n",
    "fine_tuned_path = f\"../../../local/models_llora/{fine_tuned_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c562b4c-3e2b-4ce0-b9dd-020ec89d45d6",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è  Automatic Quantization Configuration\n",
    "\n",
    "We use an intelligent selector to automatically choose the optimal quantization strategy for the hardware environment.\n",
    "\n",
    "- `QuantizationSelector()` analyzes the number of available GPUs and their memory.\n",
    "- If multiple GPUs with sufficient VRAM are detected, it applies 8-bit quantization for faster performance.\n",
    "- Otherwise, it falls back to `4-bit QLoRA` using `nf4` and double quantization to reduce memory usage.\n",
    "\n",
    "This adaptive configuration ensures efficient fine-tuning of large language models by balancing performance and hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "311e4393-0a58-403a-8137-6fd75fd4371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using 8-bit quantization (sufficient GPUs and VRAM available).\n"
     ]
    }
   ],
   "source": [
    "quantization = QuantizationSelector()\n",
    "bnb_config = quantization.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08163f23-e86a-4873-98a9-1b405c7d2065",
   "metadata": {},
   "source": [
    "### üß© PEFT Configuration (LoRA)\n",
    "\n",
    "We define the LoRA configuration using the `LoraConfig` from PEFT (Parameter-Efficient Fine-Tuning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eabcbe2-f5eb-48f5-a40e-1ab4b8f2e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=LoRATargetMapper.get_target_modules(base_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0f756-0e6b-406d-8a4e-ce139522b0f6",
   "metadata": {},
   "source": [
    "### üß† Load and Prepare Base Model for Training\n",
    "\n",
    "In this step, we load the base model and tokenizer from the local path, apply the quantization configuration (`bnb_config`), prepare it for tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a1ea031-aed8-4ec3-a5d3-63f1217f0ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Adjusting vocabulary (262145) ‚â† Model (262144)\n"
     ]
    }
   ],
   "source": [
    "model_vocab_size = AutoModelForCausalLM.from_pretrained(model_path).config.vocab_size\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "if tokenizer_vocab_size != model_vocab_size:\n",
    "    print(f\"‚ö†Ô∏è Adjusting vocabulary ({tokenizer_vocab_size}) ‚â† Model ({model_vocab_size})\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n",
    "    tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43be1d90-7d0e-42f0-8b9b-e03fd2ee99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c40902b-9ff1-4fb0-b04f-f3387784a6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Tokenizer already has a chat_template. Skipping setup_chat_format to avoid overwriting.\n"
     ]
    }
   ],
   "source": [
    "# Safely apply chat format only if tokenizer doesn't already have a chat_template\n",
    "if tokenizer.chat_template is None:\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tokenizer already has a chat_template. Skipping setup_chat_format to avoid overwriting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c845a74e-764d-4324-a64a-74c0e385ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99c22c-d97a-4f9d-a396-db890c1f1e50",
   "metadata": {},
   "source": [
    "## üìö Dataset Loader\n",
    "\n",
    "We use the [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset provided by Hugging Face.\n",
    "\n",
    "This dataset contains prompts along with two model-generated responses:\n",
    "- **chosen**: the response preferred by human annotators\n",
    "- **rejected**: the less preferred one\n",
    "\n",
    "For this experiment, we load a subset of the data to speed up training and evaluation.  \n",
    "A fixed seed ensures reproducibility when shuffling the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "812d6d5f-0877-4850-acda-f2a612ea261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=[\"train_prefs\", \"test_prefs\"])\n",
    "\n",
    "# üìä Define sample sizes for a lightweight experiment\n",
    "train_samples = 5000                         # Subset size for training\n",
    "original_train_samples = 61135              # Total training examples in the original dataset\n",
    "test_samples = int((2000 / original_train_samples) * train_samples)  # Proportional test size\n",
    "\n",
    "# üîÄ Shuffle and sample subsets from both splits\n",
    "train_subset = dataset[0].shuffle(seed=42).select(range(train_samples))\n",
    "test_subset = dataset[1].shuffle(seed=42).select(range(test_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9ddd-b459-487d-b9fa-a2ca42d8eaf2",
   "metadata": {},
   "source": [
    "### üìä Dataset Visualization\n",
    "\n",
    "To help understand how the dataset works, we use the `UltraFeedbackVisualizer`.\n",
    "\n",
    "This tool logs examples from the dataset into **TensorBoard**, including:\n",
    "- The **original prompt** given to the model\n",
    "- The two possible answers: one **preferred by humans** and one that was **rejected**\n",
    "- A simple comparison showing which response was rated better\n",
    "\n",
    "Each example is displayed with clear labels and scores to help illustrate the kinds of outputs humans value more ‚Äî **before we do any fine-tuning**.\n",
    "\n",
    "> This is useful to explore what ‚Äúgood answers‚Äù look like, based on real human feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40a8b0bd-b153-4771-a196-fd0f6b80e761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 12:44:52,048 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-23 12:44:52,050 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-04-23 12:44:55,882 - INFO - üìä Logging training samples (human feedback only)...\n",
      "2025-04-23 12:44:55,991 - INFO - [Example 0] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,023 - INFO - [Example 1] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,061 - INFO - [Example 2] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,095 - INFO - [Example 3] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,132 - INFO - [Example 4] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,169 - INFO - [Example 5] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,206 - INFO - [Example 6] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,293 - INFO - [Example 7] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,345 - INFO - [Example 8] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,374 - INFO - [Example 9] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,419 - INFO - [Example 10] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,474 - INFO - [Example 11] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,548 - INFO - [Example 12] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,587 - INFO - [Example 13] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,637 - INFO - [Example 14] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,682 - INFO - [Example 15] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,716 - INFO - [Example 16] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,761 - INFO - [Example 17] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,783 - INFO - [Example 18] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,836 - INFO - [Example 19] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,851 - INFO - üìä Logging test samples (human feedback only)...\n",
      "2025-04-23 12:44:56,899 - INFO - [Example 0] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,945 - INFO - [Example 1] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:56,976 - INFO - [Example 2] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,020 - INFO - [Example 3] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,064 - INFO - [Example 4] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,090 - INFO - [Example 5] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,115 - INFO - [Example 6] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,130 - INFO - [Example 7] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,155 - INFO - [Example 8] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,181 - INFO - [Example 9] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,202 - INFO - [Example 10] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,224 - INFO - [Example 11] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,246 - INFO - [Example 12] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,262 - INFO - [Example 13] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,285 - INFO - [Example 14] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,303 - INFO - [Example 15] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,322 - INFO - [Example 16] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,342 - INFO - [Example 17] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,362 - INFO - [Example 18] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,384 - INFO - [Example 19] ‚úÖ Logged successfully\n",
      "2025-04-23 12:44:57,442 - INFO - ‚úÖ Human feedback exploration complete!\n",
      "Launch TensorBoard with:\n",
      "tensorboard --logdir=/phoenix/tensorboard/tensorlogs --port 6006\n"
     ]
    }
   ],
   "source": [
    "visualizer = UltraFeedbackVisualizer(train_subset, test_subset,max_samples=20)\n",
    "visualizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39e3ec8a-1c9b-4d79-8960-3fdbeae10449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f75bd3efd04948bb267ef9dafd1e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3253912150f43bbae30c45e9ed90fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/163 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
      "    num_rows: 5000\n",
      "}), Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
      "    num_rows: 163\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "def process(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset[0] = train_subset.map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "dataset[1] = test_subset.map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee1788-fc5a-4afd-bb22-d5b812b41635",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è ORPO Configuration\n",
    "\n",
    "We define the training configuration using the `ORPOConfig` class from TRL (Transformers Reinforcement Learning).\n",
    "\n",
    "This configuration controls how the model will be fine-tuned using ORPO (Offline Reinforcement Preference Optimization), a technique that aligns model outputs with human preferences.\n",
    "\n",
    "Key parameters include:\n",
    "- `learning_rate`: sets how fast the model updates (8e-6 is typical for PEFT)\n",
    "- `beta`: the strength of the ORPO loss term\n",
    "- `optim`: uses 8-bit optimizer for memory efficiency (paged_adamw_8bit)\n",
    "- `max_steps`: controls how long training will run (e.g., 1000 steps)\n",
    "- `eval_strategy` and `eval_steps`: defines how and when to evaluate during training\n",
    "- `output_dir`: directory to save the trained model\n",
    "\n",
    "> This configuration is compatible with all the selected models (e.g., Mistral, LLaMA, Gemma) and optimized for QLoRA fine-tuning on consumer or research-grade GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfc31b-313d-46c7-ac16-80534efb8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 12:45:11 INFO mlflow.tracking.fluent: Experiment with name 'AIStudio-Fine-Tuning-Experiment' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('/phoenix/mlflow')\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=[\"mlflow\",\"tensorboard\"],\n",
    "    output_dir=\"./results/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d084b1-62da-4ed7-8d49-729db9bb4e3d",
   "metadata": {},
   "source": [
    "### üöÄ ORPO Trainer\n",
    "\n",
    "We now initialize the `ORPOTrainer`, which orchestrates the fine-tuning process using the Offline Reinforcement Preference Optimization (ORPO) strategy.\n",
    "\n",
    "It takes as input:\n",
    "- The **base model**, already prepared with QLoRA and chat formatting\n",
    "- The **ORPO configuration** (`orpo_args`) containing all training hyperparameters\n",
    "- The **training and evaluation datasets**\n",
    "- The **LoRA configuration** (`peft_config`) for parameter-efficient fine-tuning\n",
    "- The **tokenizer**, passed as a `processing_class`, to apply proper formatting and padding\n",
    "\n",
    "Once initialized, the trainer will be ready to start training with `trainer.train()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45ed75d4-44c8-4284-9f12-3da8f4cce525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/trl/trainer/orpo_trainer.py:275: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[0],\n",
    "    eval_dataset=dataset[1],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca638984-d847-41fd-b918-78517ba82817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  28/1000 09:05 < 5:39:58, 0.05 it/s, Epoch 0.09/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(fine_tuned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bad850f-43fc-4d88-a276-18fa70726863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up memory...\n",
      "üîÑ Loading tokenizer and base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5971232491984065973ab5be0e626b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c0b0e583084136adf1f24704aaa8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37044f5a259464f8cce581ee6f8624e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13ae99e55b0494d84ad1164b90c1ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fa38cb21384f939a89515d59337526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c02fed45d604110adb3fb5f6462d8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9100c96412094869a55758a494eef153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a63555779374a3fa7a7a22aa9204464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Resizing token embeddings: model (262144) ‚Üí tokenizer (262145)\n",
      "‚ö†Ô∏è Tokenizer already has chat_template. Skipping setup_chat_format.\n",
      "üîó Loading LoRA weights from: ./Orpo-gemma-3-1b-it-FT\n",
      "üß† Merging LoRA weights...\n",
      "üíæ Saving merged model to: ../../../local/models_llora/Orpo-gemma-3-1b-it-FT\n",
      "‚úÖ Finished! Model successfully merged and saved locally.\n"
     ]
    }
   ],
   "source": [
    "merge_lora_and_save(\n",
    "    base_model_id=MODEL,\n",
    "    finetuned_lora_path=fine_tuned_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4242ea0-b894-4852-9263-2e48eedaf7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propose a solution that could reduce the rate of deforestation, including at least 12 different examples that are specific to locations such as Amazonia, and at least 2 in 20 different regions worldwide, one of which is the European Union.user\n",
      "Propose a solution that could reduce the rate of deforestation, including at least 12 different examples that are specific to locations such as Amazonia, and at least 2 in 20 different regions worldwide, one of which is the European Union.\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_path = f\"../../../local/models_llora/{fine_tuned_name}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_path, torch_dtype=torch.float16).cuda().eval()\n",
    "\n",
    "prompt = \"Propose a solution that could reduce the rate of deforestation\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe5795-9dbf-412e-96d9-809d561c8c6c",
   "metadata": {},
   "source": [
    "## Galileo Evaluate\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo Evaluate to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys\n",
    "\n",
    "Galileo Evaluate is a platform designed to optimize and simplify the experimentation and evaluation of generative AI systems, especially large language model (LLM) applications. Its goal is to facilitate the process of building AI systems with deep insights and collaborative tools, replacing fragmented experimentation in spreadsheets and notebooks with a more integrated approach.\n",
    "\n",
    "You can log metrics in Galileo Evaluate and track all your experiments in one place. In our example, we logged several questions, selected specific metrics, and ran a batch of experiments to evaluate our chain. To learn more about the available metrics, see: Galileo Guardrail Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9bff3e-0c0b-4eca-a10b-a086593d834d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã You have logged into üî≠ Galileo (https://console.hp.galileocloud.io/) as diogo.vieira@hp.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(console_url=HttpUrl('https://console.hp.galileocloud.io/'), username=None, password=None, api_key=SecretStr('**********'), token=SecretStr('**********'), current_user='diogo.vieira@hp.com', current_project_id=None, current_project_name=None, current_run_id=None, current_run_name=None, current_run_url=None, current_run_task_type=None, current_template_id=None, current_template_name=None, current_template_version_id=None, current_template_version=None, current_template=None, current_dataset_id=None, current_job_id=None, current_prompt_optimization_job_id=None, api_url=HttpUrl('https://api.hp.galileocloud.io/'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################\n",
    "# In order to connect to Galileo, create a secrets.yaml file in the configs folder.\n",
    "# This file should be an entry called GALILEO_API_KEY, with your personal Galileo API Key\n",
    "# Galileo API keys can be created on https://console.hp.galileocloud.io/settings/api-keys\n",
    "#########################################\n",
    "\n",
    "setup_galileo_environment(secrets)\n",
    "pq.login(os.environ['GALILEO_CONSOLE_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bf71f8e-0e1a-4732-8f6f-1c62d128d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AcceleratedInferenceRunner:üîÑ Loading tokenizer and base model from ModelSelector...\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Model loaded and ready for inference.\n",
      "INFO:AcceleratedInferenceRunner:üîÑ Loading tokenizer and base model from ModelSelector...\n",
      "INFO:AcceleratedInferenceRunner:üéØ Applying LoRA fine-tuned weights...\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Model loaded and ready for inference.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Explain the importance of sustainable agriculture....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Running prompt 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Explain the importance of sustainable agriculture....\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Write a Python function to check for palindromes....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Running prompt 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:AcceleratedInferenceRunner:üîç Running inference for prompt (truncated): Write a Python function to check for palindromes....\n",
      "INFO:AcceleratedInferenceRunner:‚úÖ Inference complete.\n",
      "INFO:promptquality.utils.logger:Project AIStudio-Fine-Tuning-Evaluate already exists, using it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d65ee56a0f4ee59c2d03d11b8c13a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Done ‚úÖ\n",
      "instruction_adherence: Computing üöß\n",
      "cost: Done ‚úÖ\n",
      "toxicity: Done ‚úÖ\n",
      "pii: Done ‚úÖ\n",
      "protect_status: Done ‚úÖ\n",
      "latency: Done ‚úÖ\n",
      "factuality: Computing üöß\n",
      "üî≠ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/48f07fac-cf49-4fb0-ba74-c4c109edd1e4/550cfb8a-70e6-468f-baf2-93e1b7f4b265?taskType=12\n",
      "‚úÖ Finished logging outputs for both models to Galileo.\n"
     ]
    }
   ],
   "source": [
    "comparer = GalileoLocalComparer(\n",
    "    base_selector=selector,\n",
    "    finetuned_path=fine_tuned_path,\n",
    "    prompts=[\n",
    "        \"Explain the importance of sustainable agriculture.\",\n",
    "        \"Write a Python function to check for palindromes.\",\n",
    "    ],\n",
    "    galileo_project_name=GALILEO_EVALUATE_PROJECT_NAME,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "comparer.compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391ccbf-ca08-4299-93c0-f67fb2551a59",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
